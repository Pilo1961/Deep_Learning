{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copia de StackGan_jp.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPky9U7GXXyZtgFDy4CoJjl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pilo1961/Deep_Learning/blob/master/Copia_de_StackGan_jp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LKwcCMHqrsc1",
        "colab_type": "code",
        "outputId": "22f96b65-019b-4d1f-e153-fc0209b74751",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "# mount drive \n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nrGnmynhB4xR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7d9a26eb-1e18-44c6-fd74-c3441a2f9be7"
      },
      "source": [
        "# importa modulos propios\n",
        "import sys\n",
        "sys.path.append('/content/drive/My Drive/')\n",
        "\n",
        "import model\n",
        "#import util"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rNqBV38ewQjx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from skimage.io import imread, imsave\n",
        "from skimage.transform import rescale\n",
        "from skimage import img_as_ubyte\n",
        "import pickle\n",
        "import datetime as dt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oNyfxXF1wfj_",
        "colab_type": "text"
      },
      "source": [
        "Especificamo hiperparametros"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EMOso4WGwU3g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_dir = '/content/drive/My Drive'\n",
        "train_dir = data_dir + \"/train_64\"\n",
        "test_dir = data_dir + \"/test_64\"\n",
        "image_size = 64\n",
        "batch_size = 64\n",
        "z_dim = 100\n",
        "stage1_generator_lr = 0.0002\n",
        "stage1_discriminator_lr = 0.0002\n",
        "stage1_lr_decay_step = 600\n",
        "epochs = 1000\n",
        "condition_dim = 128\n",
        "\n",
        "#embeddings_file_path_train = train_dir + \"/char-CNN-RNN-embeddings.pickle\"\n",
        "#embeddings_file_path_test = test_dir + \"/char-CNN-RNN-embeddings.pickle\"\n",
        "\n",
        "#filenames_file_path_train = train_dir + \"/filenames.pickle\"\n",
        "#filenames_file_path_test = test_dir + \"/filenames.pickle\"\n",
        "\n",
        "#class_info_file_path_train = train_dir + \"/class_info.pickle\"\n",
        "#class_info_file_path_test = test_dir + \"/class_info.pickle\"\n",
        "\n",
        "#cub_dataset_dir = data_dir + \"/CUB_200_2011\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FgQ8ok-a7vC6",
        "colab_type": "text"
      },
      "source": [
        "# Load data\n",
        "## Image list"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDkDKDkgzsyl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#esto se debe de ir al modulo util\n",
        "def add_fileName(df):\n",
        "  '''\n",
        "    Add filename column to the ID-description list\n",
        "  '''\n",
        "  df['filename']='a'\n",
        "  for index, row in df.iterrows():\n",
        "      try:\n",
        "        new_name = row['ID'][:-6] + '_' + row['ID'][-1] + '.jpg'\n",
        "        row['filename']=new_name\n",
        "      except:\n",
        "        found_n.append(row['ID'])\n",
        "\n",
        "  return df\n",
        "\n",
        "def train_test(df):\n",
        "  x_train = df[df.index % 5 != 0]     # Excludes every 5th row starting from 0\n",
        "  x_test = df[df.index % 5 == 0]      # Selects every 5th row starting from 0\n",
        "  #print(x_train.shape)\n",
        "  #print(x_test.shape)  \n",
        "  return x_train, x_test\n",
        "\n",
        "\n",
        "# Load image\n",
        "def load_image(img_id,src='Flicker8k_Dataset/'):\n",
        "    I = imread('/content/drive/My Drive/'+src+img_id)\n",
        "    #I = margin_img(I)\n",
        "    return I\n",
        "\n",
        "# Hacemos un pickle que tiene un arreglo de numpy con toda la informacion de las imagenes\n",
        "# ojo:\n",
        "#file not found:  2258277193_586949ec62.j_1.jpg\n",
        "#file not found:  2258277193_586949ec62.j_2.jpg\n",
        "#file not found:  2258277193_586949ec62.j_3.jpg\n",
        "#file not found:  2258277193_586949ec62.j_4.jpg\n",
        "\n",
        "def img_train_pickle(x_train):\n",
        "  src='test_64/'\n",
        "  img_train=[]\n",
        "  print(\"Images to load: \", len(x_train[\"filename\"]))\n",
        "  for i, img_name in enumerate(x_train[\"filename\"]):\n",
        "    try:\n",
        "      img=load_image(img_name,src)\n",
        "      img_train.append(img)\n",
        "    except:\n",
        "      print(\"file not found: \",img_name)\n",
        "    if i%3000==0: print(\"Loaded images: \",i)\n",
        "  \n",
        "  img_train=np.array(img_train)\n",
        "  \n",
        "  with open(\"/content/drive/My Drive/img_test_data.pkl\", 'wb') as f:\n",
        "    pickle.dump(img_train,f) \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WY52LMsuzs1U",
        "colab_type": "code",
        "outputId": "054c5fdd-e435-4a45-f0c9-e336485d3cdf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "# Load data\n",
        "df = pd.read_csv(\"/content/drive/My Drive/Flickr8k.token.txt\", sep='\\t', names=['ID', 'Text'], index_col=False)\n",
        "df=add_fileName(df)\n",
        "print(df.shape)\n",
        "\n",
        "# Remove entries with nan values\n",
        "df.dropna(inplace=True)\n",
        "df.isnull().values.any()\n",
        "df.head()\n",
        "\n",
        "#Quito los registros de una imagen que falta\n",
        "df.drop([6730,6731,6732,6733,6734],inplace=True)\n",
        "df.reset_index()\n",
        "df.shape"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(40460, 3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(40455, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-v6oPqorCQgj",
        "colab_type": "text"
      },
      "source": [
        "# Embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_wFVCDnzs4L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "with open(\"/content/drive/My Drive/embeddings_jp/infersent_1024_encoding.pkl\", 'rb') as f:\n",
        "  embedding = pickle.load(f) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k8YrhJk8DGI3",
        "colab_type": "code",
        "outputId": "2a7f66af-4dba-46ab-9baa-597f97514326",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "source": [
        "print(embedding.shape)\n",
        "embedding[:5]"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(32364, 1024)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.00719924, 0.29595286, 0.25099796, ..., 0.5558008 , 0.29030454,\n",
              "        0.28820118],\n",
              "       [0.5907835 , 0.18375209, 0.30119446, ..., 0.867978  , 0.        ,\n",
              "        0.08150935],\n",
              "       [0.42197692, 0.15705526, 0.19881831, ..., 0.8628484 , 0.07011686,\n",
              "        0.08546458],\n",
              "       [0.        , 0.1086157 , 0.24848017, ..., 0.5315687 , 0.25133806,\n",
              "        0.24955702],\n",
              "       [0.        , 0.12755851, 0.31044996, ..., 0.6411171 , 0.11928789,\n",
              "        0.8201335 ]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X32yOjfFEl_Q",
        "colab_type": "code",
        "outputId": "1c28fe86-5471-49ec-fb20-8207288a39fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        }
      },
      "source": [
        "df_emb=pd.DataFrame(embedding)\n",
        "df_emb.head()\n",
        "\n",
        "#solo correr esto si se necesita borrar la foto que no esta\n",
        "#Quito los registros de una imagen que falta\n",
        "#df_emb.drop([6730,6731,6732,6733,6734],inplace=True)\n",
        "#df_emb.reset_index()\n",
        "#df_emb.shape"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>2008</th>\n",
              "      <th>2009</th>\n",
              "      <th>2010</th>\n",
              "      <th>2011</th>\n",
              "      <th>2012</th>\n",
              "      <th>2013</th>\n",
              "      <th>2014</th>\n",
              "      <th>2015</th>\n",
              "      <th>2016</th>\n",
              "      <th>2017</th>\n",
              "      <th>2018</th>\n",
              "      <th>2019</th>\n",
              "      <th>2020</th>\n",
              "      <th>2021</th>\n",
              "      <th>2022</th>\n",
              "      <th>2023</th>\n",
              "      <th>2024</th>\n",
              "      <th>2025</th>\n",
              "      <th>2026</th>\n",
              "      <th>2027</th>\n",
              "      <th>2028</th>\n",
              "      <th>2029</th>\n",
              "      <th>2030</th>\n",
              "      <th>2031</th>\n",
              "      <th>2032</th>\n",
              "      <th>2033</th>\n",
              "      <th>2034</th>\n",
              "      <th>2035</th>\n",
              "      <th>2036</th>\n",
              "      <th>2037</th>\n",
              "      <th>2038</th>\n",
              "      <th>2039</th>\n",
              "      <th>2040</th>\n",
              "      <th>2041</th>\n",
              "      <th>2042</th>\n",
              "      <th>2043</th>\n",
              "      <th>2044</th>\n",
              "      <th>2045</th>\n",
              "      <th>2046</th>\n",
              "      <th>2047</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.746836</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.931276</td>\n",
              "      <td>1.447990</td>\n",
              "      <td>1.356762</td>\n",
              "      <td>0.859256</td>\n",
              "      <td>1.282008</td>\n",
              "      <td>1.385121</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.563787</td>\n",
              "      <td>1.454054</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.683455</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.750486</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.337395</td>\n",
              "      <td>0.639892</td>\n",
              "      <td>1.518962</td>\n",
              "      <td>0.053421</td>\n",
              "      <td>1.715396</td>\n",
              "      <td>1.283217</td>\n",
              "      <td>1.541835</td>\n",
              "      <td>1.218065</td>\n",
              "      <td>0.224575</td>\n",
              "      <td>0.792906</td>\n",
              "      <td>1.139164</td>\n",
              "      <td>0.052395</td>\n",
              "      <td>0.704676</td>\n",
              "      <td>1.610687</td>\n",
              "      <td>0.451876</td>\n",
              "      <td>0.839882</td>\n",
              "      <td>0.427703</td>\n",
              "      <td>0.858548</td>\n",
              "      <td>0.652197</td>\n",
              "      <td>1.294270</td>\n",
              "      <td>1.595016</td>\n",
              "      <td>0.978071</td>\n",
              "      <td>...</td>\n",
              "      <td>0.684472</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.974220</td>\n",
              "      <td>1.389602</td>\n",
              "      <td>0.356755</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.801388</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.224353</td>\n",
              "      <td>0.531555</td>\n",
              "      <td>1.355668</td>\n",
              "      <td>0.439719</td>\n",
              "      <td>1.063712</td>\n",
              "      <td>1.217662</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.899856</td>\n",
              "      <td>1.342100</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.758532</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.292588</td>\n",
              "      <td>1.471911</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.148974</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.275467</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.097314</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.740773</td>\n",
              "      <td>0.468888</td>\n",
              "      <td>1.299500</td>\n",
              "      <td>0.926064</td>\n",
              "      <td>1.264413</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.813168</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>2.087718</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.209995</td>\n",
              "      <td>1.819482</td>\n",
              "      <td>1.624871</td>\n",
              "      <td>1.034178</td>\n",
              "      <td>1.612795</td>\n",
              "      <td>1.738789</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.029149</td>\n",
              "      <td>1.763703</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.818733</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.908369</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.632864</td>\n",
              "      <td>0.773741</td>\n",
              "      <td>1.784528</td>\n",
              "      <td>0.017366</td>\n",
              "      <td>2.041179</td>\n",
              "      <td>1.505827</td>\n",
              "      <td>1.960786</td>\n",
              "      <td>1.596756</td>\n",
              "      <td>0.242113</td>\n",
              "      <td>0.999698</td>\n",
              "      <td>1.410002</td>\n",
              "      <td>0.067680</td>\n",
              "      <td>0.876008</td>\n",
              "      <td>1.885961</td>\n",
              "      <td>0.540933</td>\n",
              "      <td>1.076209</td>\n",
              "      <td>0.493560</td>\n",
              "      <td>0.937256</td>\n",
              "      <td>0.942777</td>\n",
              "      <td>1.687483</td>\n",
              "      <td>1.907183</td>\n",
              "      <td>1.297547</td>\n",
              "      <td>...</td>\n",
              "      <td>0.841416</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.137444</td>\n",
              "      <td>1.647426</td>\n",
              "      <td>0.587025</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.838547</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.505699</td>\n",
              "      <td>0.635895</td>\n",
              "      <td>1.663432</td>\n",
              "      <td>0.493379</td>\n",
              "      <td>1.366974</td>\n",
              "      <td>1.302853</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.066198</td>\n",
              "      <td>1.698142</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.822328</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.486578</td>\n",
              "      <td>1.935515</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.462019</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.440314</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.382626</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.259202</td>\n",
              "      <td>0.593171</td>\n",
              "      <td>1.575310</td>\n",
              "      <td>1.160255</td>\n",
              "      <td>1.512525</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.966996</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>2.051764</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.166297</td>\n",
              "      <td>1.722983</td>\n",
              "      <td>1.498636</td>\n",
              "      <td>0.951488</td>\n",
              "      <td>1.551099</td>\n",
              "      <td>1.629975</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.938318</td>\n",
              "      <td>1.621363</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.731320</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.904826</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.485968</td>\n",
              "      <td>0.702762</td>\n",
              "      <td>1.617838</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.881845</td>\n",
              "      <td>1.420786</td>\n",
              "      <td>1.809913</td>\n",
              "      <td>1.454543</td>\n",
              "      <td>0.246779</td>\n",
              "      <td>0.994074</td>\n",
              "      <td>1.356247</td>\n",
              "      <td>0.090851</td>\n",
              "      <td>0.873286</td>\n",
              "      <td>1.856803</td>\n",
              "      <td>0.503457</td>\n",
              "      <td>1.110348</td>\n",
              "      <td>0.515202</td>\n",
              "      <td>0.857243</td>\n",
              "      <td>0.873388</td>\n",
              "      <td>1.420393</td>\n",
              "      <td>1.718299</td>\n",
              "      <td>1.259737</td>\n",
              "      <td>...</td>\n",
              "      <td>0.707185</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.130338</td>\n",
              "      <td>1.550914</td>\n",
              "      <td>0.597504</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.900907</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.403504</td>\n",
              "      <td>0.676798</td>\n",
              "      <td>1.556562</td>\n",
              "      <td>0.498200</td>\n",
              "      <td>1.291022</td>\n",
              "      <td>1.221080</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.988663</td>\n",
              "      <td>1.615499</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.810549</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.518305</td>\n",
              "      <td>1.751477</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.418914</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.398941</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.327650</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.110633</td>\n",
              "      <td>0.564648</td>\n",
              "      <td>1.594213</td>\n",
              "      <td>1.078357</td>\n",
              "      <td>1.440756</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.951599</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>2.464272</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.357983</td>\n",
              "      <td>1.922686</td>\n",
              "      <td>1.777479</td>\n",
              "      <td>0.997722</td>\n",
              "      <td>1.896735</td>\n",
              "      <td>1.889828</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.132794</td>\n",
              "      <td>1.997364</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.847513</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.946881</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.837438</td>\n",
              "      <td>0.904739</td>\n",
              "      <td>2.165217</td>\n",
              "      <td>0.033110</td>\n",
              "      <td>2.309935</td>\n",
              "      <td>1.757875</td>\n",
              "      <td>1.928151</td>\n",
              "      <td>1.643613</td>\n",
              "      <td>0.219728</td>\n",
              "      <td>0.917767</td>\n",
              "      <td>1.531009</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.002409</td>\n",
              "      <td>2.104956</td>\n",
              "      <td>0.417289</td>\n",
              "      <td>1.095618</td>\n",
              "      <td>0.464313</td>\n",
              "      <td>1.107965</td>\n",
              "      <td>0.953642</td>\n",
              "      <td>1.884227</td>\n",
              "      <td>2.034115</td>\n",
              "      <td>1.235775</td>\n",
              "      <td>...</td>\n",
              "      <td>0.935780</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.038223</td>\n",
              "      <td>1.810159</td>\n",
              "      <td>0.529604</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.005254</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.644976</td>\n",
              "      <td>0.613825</td>\n",
              "      <td>1.618092</td>\n",
              "      <td>0.424530</td>\n",
              "      <td>1.285653</td>\n",
              "      <td>1.538418</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.166991</td>\n",
              "      <td>1.931877</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.038275</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.573743</td>\n",
              "      <td>2.078988</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.579780</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.516795</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.488402</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.503132</td>\n",
              "      <td>0.537330</td>\n",
              "      <td>1.592172</td>\n",
              "      <td>1.049968</td>\n",
              "      <td>1.648481</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.835587</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>2.688271</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.541515</td>\n",
              "      <td>2.034183</td>\n",
              "      <td>1.943247</td>\n",
              "      <td>1.120744</td>\n",
              "      <td>2.099424</td>\n",
              "      <td>1.916724</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.281366</td>\n",
              "      <td>2.043264</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.664024</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.031136</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.037824</td>\n",
              "      <td>1.045331</td>\n",
              "      <td>2.171372</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.349773</td>\n",
              "      <td>1.871091</td>\n",
              "      <td>2.087316</td>\n",
              "      <td>1.662914</td>\n",
              "      <td>0.319845</td>\n",
              "      <td>1.050390</td>\n",
              "      <td>1.747847</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.081504</td>\n",
              "      <td>2.164313</td>\n",
              "      <td>0.308654</td>\n",
              "      <td>1.375295</td>\n",
              "      <td>0.511467</td>\n",
              "      <td>1.117464</td>\n",
              "      <td>0.888759</td>\n",
              "      <td>1.980708</td>\n",
              "      <td>2.178228</td>\n",
              "      <td>1.314319</td>\n",
              "      <td>...</td>\n",
              "      <td>0.891186</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.250742</td>\n",
              "      <td>1.849244</td>\n",
              "      <td>0.617844</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.939353</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.940879</td>\n",
              "      <td>0.675314</td>\n",
              "      <td>1.699824</td>\n",
              "      <td>0.406623</td>\n",
              "      <td>1.283965</td>\n",
              "      <td>1.594314</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.195205</td>\n",
              "      <td>2.059218</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.181809</td>\n",
              "      <td>1.106166</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.886027</td>\n",
              "      <td>2.075312</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.993412</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.619841</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.649752</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.571683</td>\n",
              "      <td>0.588482</td>\n",
              "      <td>1.670402</td>\n",
              "      <td>1.154218</td>\n",
              "      <td>1.618534</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.991953</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 2048 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   0         1     2         3     ...      2044      2045  2046      2047\n",
              "0   0.0  1.746836   0.0  0.931276  ...  0.926064  1.264413   0.0  0.813168\n",
              "1   0.0  2.087718   0.0  1.209995  ...  1.160255  1.512525   0.0  0.966996\n",
              "2   0.0  2.051764   0.0  1.166297  ...  1.078357  1.440756   0.0  0.951599\n",
              "3   0.0  2.464272   0.0  1.357983  ...  1.049968  1.648481   0.0  0.835587\n",
              "4   0.0  2.688271   0.0  1.541515  ...  1.154218  1.618534   0.0  0.991953\n",
              "\n",
              "[5 rows x 2048 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JyE5zirIDtzS",
        "colab_type": "text"
      },
      "source": [
        "# Train test split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-8wXURlDGMX",
        "colab_type": "code",
        "outputId": "229ba919-bb32-4f29-a74d-2b0020abdad6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        }
      },
      "source": [
        "x_train, x_test= train_test(df)\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)\n",
        "x_train.head()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(32364, 3)\n",
            "(8091, 3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>Text</th>\n",
              "      <th>filename</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1000268201_693b08cb0e.jpg#1</td>\n",
              "      <td>A girl going into a wooden building .</td>\n",
              "      <td>1000268201_693b08cb0e_1.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1000268201_693b08cb0e.jpg#2</td>\n",
              "      <td>A little girl climbing into a wooden playhouse .</td>\n",
              "      <td>1000268201_693b08cb0e_2.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1000268201_693b08cb0e.jpg#3</td>\n",
              "      <td>A little girl climbing the stairs to her playh...</td>\n",
              "      <td>1000268201_693b08cb0e_3.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1000268201_693b08cb0e.jpg#4</td>\n",
              "      <td>A little girl in a pink dress going into a woo...</td>\n",
              "      <td>1000268201_693b08cb0e_4.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1001773457_577c3a7d70.jpg#1</td>\n",
              "      <td>A black dog and a tri-colored dog playing with...</td>\n",
              "      <td>1001773457_577c3a7d70_1.jpg</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                            ID  ...                     filename\n",
              "1  1000268201_693b08cb0e.jpg#1  ...  1000268201_693b08cb0e_1.jpg\n",
              "2  1000268201_693b08cb0e.jpg#2  ...  1000268201_693b08cb0e_2.jpg\n",
              "3  1000268201_693b08cb0e.jpg#3  ...  1000268201_693b08cb0e_3.jpg\n",
              "4  1000268201_693b08cb0e.jpg#4  ...  1000268201_693b08cb0e_4.jpg\n",
              "6  1001773457_577c3a7d70.jpg#1  ...  1001773457_577c3a7d70_1.jpg\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWEYHagWEP0V",
        "colab_type": "code",
        "outputId": "a3490016-e7c6-448b-d1f7-4d6f7e2d273e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "emb_train, emb_test= train_test(df_emb)\n",
        "print(emb_train.shape)\n",
        "print(emb_test.shape)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(32364, 4096)\n",
            "(8091, 4096)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SmunFw6tEQXd",
        "colab_type": "text"
      },
      "source": [
        "# Prepara imagenes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1qm4A_My9m3z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  with open(\"/content/drive/My Drive/img_train_data.pkl\", 'rb') as f:\n",
        "    img_train=pickle.load(f) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPjlDZoFvzCy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "17835e5a-06de-4cc6-e766-244327ac22f4"
      },
      "source": [
        "len(img_train)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "32364"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MSWhDygs1qmU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d594b313-7167-4625-ee6b-9e54e03af15d"
      },
      "source": [
        "# Tenemos las dimensiones correctas\n",
        "len(img_train)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "32364"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3YR88lIBztP6",
        "colab_type": "text"
      },
      "source": [
        "# Creating models\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4J4j_04oz38t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.losses import binary_crossentropy\n",
        "from keras.optimizers import adam\n",
        "from keras import backend as K\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "import tensorflow as tf\n",
        "tf.config.experimental_run_functions_eagerly(True)\n",
        "from tensorflow.keras.layers import concatenate, Embedding, Dense, Dropout, Reshape, UpSampling2D, Conv2D, BatchNormalization, Activation, Input, Concatenate, LeakyReLU, Flatten, Lambda"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HqSj5dIkzva6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dis_optimizer = adam(lr=stage1_discriminator_lr, beta_1=0.5, beta_2=0.999)\n",
        "gen_optimizer = adam(lr=stage1_generator_lr, beta_1=0.5, beta_2=0.999)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxIpCGnX2cL9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def KL_loss(y_true, y_pred):\n",
        "    mean = y_pred[:, :128]\n",
        "    logsigma = y_pred[:, :128]\n",
        "    loss = -logsigma + .5 * (-1 + K.exp(2. * logsigma) + K.square(mean))\n",
        "    loss = K.mean(loss)\n",
        "    return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFshF-ZSzycK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ca_model = model.create_CA_model(len_embedding=1024)\n",
        "ca_model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\")\n",
        "\n",
        "stage1_dis = model.create_disc_st1()\n",
        "stage1_dis.compile(loss='binary_crossentropy', optimizer=\"adam\")#dis_optimizer)\n",
        "\n",
        "stage1_gen = model.create_gen_st1(len_embedding=1024)\n",
        "stage1_gen.compile(loss=\"mse\", optimizer=\"adam\")#gen_optimizer)\n",
        "\n",
        "#embedding_compressor_model = create_embedding_compressor()\n",
        "#embedding_compressor_model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\")\n",
        "\n",
        "adversarial_model = model.create_adversarial_model(gen=stage1_gen, disc=stage1_dis,len_embedding=1024)\n",
        "adversarial_model.compile(loss=['binary_crossentropy', KL_loss], loss_weights=[1, 2.0],\n",
        "                          optimizer=\"adam\", metrics=None)#gen_optimizer, metrics=None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKhtEMf9GAh1",
        "colab_type": "text"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRt9Kq2uvkOb",
        "colab_type": "text"
      },
      "source": [
        "Earlier, label/target values for a classifier were 0 or 1; 0 for fake images and 1 for real images. Because of this, GANs were prone to adversarial examples, which are inputs to a neural network that result in an incorrect output from the network. Label smoothing is an approach to provide smoothed labels to the discriminator network. This means we can have decimal values such as 0.9 (true), 0.8 (true), 0.1 (fake), or 0.2 (fake), instead of labeling every example as either 1 (true) or 0 (fake). We smooth the target values (label values) of the real images as well as of the fake images. Label smoothing can reduce the risk of adversarial examples in GANs. To apply label smoothing, assign the labels 0.9, 0.8, and 0.7, and 0.1, 0.2, and 0.3, to the images. To find out more about label smoothing, refer to the following paper.\n",
        "\n",
        "[Improved techniques for training GAN](https://arxiv.org/pdf/1606.03498.pdf)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33c4gr7A2PGq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zap5o1glMZAI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ae970c5a-244d-4209-9e55-dcb7c6cff635"
      },
      "source": [
        "#emb_train=emb_train.to_numpy()\n",
        "emb_train=embedding\n",
        "emb_train.shape"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(32364, 1024)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Ew0otTFGGoj",
        "colab_type": "code",
        "outputId": "9d0aeac3-216a-42aa-b820-7a3423e8ddd6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "batch_size=64\n",
        "epoch=50\n",
        "\n",
        "real_labels = np.ones((batch_size, 1), dtype=float) * 0.9\n",
        "fake_labels = np.zeros((batch_size, 1), dtype=float) * 0.1\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  print(\"========================================\")\n",
        "  print(\"Epoch is:\", epoch)\n",
        "  print(\"Number of batches\", int(img_train.shape[0] / batch_size))\n",
        "\n",
        "  gen_losses = []\n",
        "  dis_losses = []\n",
        "\n",
        "  number_of_batches = int(img_train.shape[0] / batch_size)\n",
        "  for index in range(number_of_batches):\n",
        "    #print(\"Batch:{}\".format(index+1))\n",
        "\n",
        "     # Create a batch of noise vectors\n",
        "    z_noise = np.random.normal(0, 1, size=(batch_size, z_dim))\n",
        "    image_batch = img_train[index * batch_size:(index + 1) * batch_size]\n",
        "    embedding_batch = emb_train[index * batch_size:(index + 1) * batch_size]\n",
        "\n",
        "    # Normalize images\n",
        "    image_batch = image_batch/255 #(image_batch - 127.5) / 127.5\n",
        "\n",
        "    fake_images, _ = stage1_gen.predict([embedding_batch, z_noise], verbose=3)\n",
        "    \n",
        "    #embedding compression\n",
        "    compressed_embedding = np.hstack((embedding_batch,embedding_batch))\n",
        "    compressed_embedding = np.reshape(compressed_embedding, (batch_size, 4, 4, condition_dim))\n",
        "\n",
        "    #calc losses\n",
        "    dis_loss_real = stage1_dis.train_on_batch([image_batch, compressed_embedding],np.reshape(real_labels, (batch_size, 1)))\n",
        "    dis_loss_fake = stage1_dis.train_on_batch([fake_images, compressed_embedding],np.reshape(fake_labels, (batch_size, 1)))\n",
        "    dis_loss_wrong = stage1_dis.train_on_batch([image_batch[:(batch_size - 1)], compressed_embedding[1:]],np.reshape(fake_labels[1:], (batch_size-1, 1)))\n",
        "\n",
        "    g_loss = adversarial_model.train_on_batch([embedding_batch, z_noise, compressed_embedding],[K.ones((batch_size, 1)) * 0.9, K.ones((batch_size, 256)) * 0.9])\n",
        "\n",
        "    d_loss = 0.5 * np.add(dis_loss_real, 0.5 * np.add(dis_loss_wrong, dis_loss_fake))\n",
        "\n",
        "    if index%100 ==0:\n",
        "      print(\"Batch:{}\".format(index+1))\n",
        "      print(\"d_loss:{}\".format(d_loss))\n",
        "            \n",
        "      print(\"g_loss:{}\".format(g_loss))\n",
        "\n",
        "    dis_losses.append(d_loss)\n",
        "    gen_losses.append(g_loss)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "========================================\n",
            "Epoch is: 0\n",
            "Number of batches 505\n",
            "Batch:1\n",
            "d_loss:6.790207892656326\n",
            "g_loss:[1.9276200532913208, 0.33735978603363037, 0.7951301336288452]\n",
            "Batch:101\n",
            "d_loss:0.614503797609359\n",
            "g_loss:[1.5474025011062622, 1.4953705072402954, 0.026015974581241608]\n",
            "Batch:201\n",
            "d_loss:0.6895494111813605\n",
            "g_loss:[1.0857374668121338, 1.0500519275665283, 0.017842750996351242]\n",
            "Batch:301\n",
            "d_loss:0.5551434475637507\n",
            "g_loss:[0.8775706887245178, 0.8452175855636597, 0.016176562756299973]\n",
            "Batch:401\n",
            "d_loss:0.5858903607877437\n",
            "g_loss:[0.9433809518814087, 0.9161632657051086, 0.013608831912279129]\n",
            "Batch:501\n",
            "d_loss:0.577331855225566\n",
            "g_loss:[0.8125073313713074, 0.7889384627342224, 0.011784447357058525]\n",
            "========================================\n",
            "Epoch is: 1\n",
            "Number of batches 505\n",
            "Batch:1\n",
            "d_loss:0.6045834853011911\n",
            "g_loss:[0.7950640320777893, 0.772757351398468, 0.011153333820402622]\n",
            "Batch:101\n",
            "d_loss:0.5817941618443001\n",
            "g_loss:[0.7483903169631958, 0.7264274954795837, 0.010981416329741478]\n",
            "Batch:201\n",
            "d_loss:0.666309442882266\n",
            "g_loss:[0.9171547293663025, 0.898461103439331, 0.009346826002001762]\n",
            "Batch:301\n",
            "d_loss:0.5545437537166436\n",
            "g_loss:[0.8153409957885742, 0.7974377870559692, 0.008951593190431595]\n",
            "Batch:401\n",
            "d_loss:0.5968992190319113\n",
            "g_loss:[0.8701704740524292, 0.8542954921722412, 0.007937492802739143]\n",
            "Batch:501\n",
            "d_loss:0.5858590443967842\n",
            "g_loss:[0.892475426197052, 0.8789540529251099, 0.006760678254067898]\n",
            "========================================\n",
            "Epoch is: 2\n",
            "Number of batches 505\n",
            "Batch:1\n",
            "d_loss:0.5948484100954374\n",
            "g_loss:[0.9320052266120911, 0.918261706829071, 0.006871750578284264]\n",
            "Batch:101\n",
            "d_loss:0.5693665731268993\n",
            "g_loss:[0.7711991667747498, 0.7579305171966553, 0.006634336430579424]\n",
            "Batch:201\n",
            "d_loss:0.5923990586525179\n",
            "g_loss:[0.8969621658325195, 0.8847705721855164, 0.006095791235566139]\n",
            "Batch:301\n",
            "d_loss:0.5491029737167992\n",
            "g_loss:[0.7440892457962036, 0.7328829765319824, 0.005603143014013767]\n",
            "Batch:401\n",
            "d_loss:0.5652206518334424\n",
            "g_loss:[0.8073756098747253, 0.7970191240310669, 0.005178249441087246]\n",
            "Batch:501\n",
            "d_loss:0.5751064653159119\n",
            "g_loss:[0.9762023687362671, 0.9674385786056519, 0.004381909966468811]\n",
            "========================================\n",
            "Epoch is: 3\n",
            "Number of batches 505\n",
            "Batch:1\n",
            "d_loss:0.5764117157468718\n",
            "g_loss:[0.8089526891708374, 0.7997927665710449, 0.004579958505928516]\n",
            "Batch:101\n",
            "d_loss:0.5972200646132251\n",
            "g_loss:[0.8166117072105408, 0.8078211545944214, 0.004395277239382267]\n",
            "Batch:201\n",
            "d_loss:0.5743377910339404\n",
            "g_loss:[0.749876081943512, 0.7415561079978943, 0.004159990698099136]\n",
            "Batch:301\n",
            "d_loss:0.5467865377358976\n",
            "g_loss:[0.8163912892341614, 0.8089351654052734, 0.0037280740216374397]\n",
            "Batch:401\n",
            "d_loss:0.5932276137173176\n",
            "g_loss:[1.0509763956069946, 1.0439069271087646, 0.0035347393713891506]\n",
            "Batch:501\n",
            "d_loss:0.5514068362035687\n",
            "g_loss:[0.815864622592926, 0.8098267316818237, 0.0030189319513738155]\n",
            "========================================\n",
            "Epoch is: 4\n",
            "Number of batches 505\n",
            "Batch:1\n",
            "d_loss:0.5707278962363489\n",
            "g_loss:[0.6652663946151733, 0.658884584903717, 0.0031908974051475525]\n",
            "Batch:101\n",
            "d_loss:0.5723619053612197\n",
            "g_loss:[0.8203216195106506, 0.8142259120941162, 0.0030478599946945906]\n",
            "Batch:201\n",
            "d_loss:0.5927835995862551\n",
            "g_loss:[0.6316218376159668, 0.6258230209350586, 0.002899416256695986]\n",
            "Batch:301\n",
            "d_loss:0.562179480680129\n",
            "g_loss:[0.9050495028495789, 0.8997847437858582, 0.002632377902045846]\n",
            "Batch:401\n",
            "d_loss:0.5387666288534092\n",
            "g_loss:[0.7374967336654663, 0.7324298620223999, 0.00253343116492033]\n",
            "Batch:501\n",
            "d_loss:0.5467223538107646\n",
            "g_loss:[0.8441572189331055, 0.8398245573043823, 0.0021663405932486057]\n",
            "========================================\n",
            "Epoch is: 5\n",
            "Number of batches 505\n",
            "Batch:1\n",
            "d_loss:0.5826804156031358\n",
            "g_loss:[0.7772374153137207, 0.7726516723632812, 0.0022928579710423946]\n",
            "Batch:101\n",
            "d_loss:0.5479270133664613\n",
            "g_loss:[0.8041626811027527, 0.7997727394104004, 0.0021949629299342632]\n",
            "Batch:201\n",
            "d_loss:0.5510508380838814\n",
            "g_loss:[0.8161394596099854, 0.811957836151123, 0.0020908010192215443]\n",
            "Batch:301\n",
            "d_loss:0.5798196024552453\n",
            "g_loss:[0.834901750087738, 0.8310515880584717, 0.001925092888996005]\n",
            "Batch:401\n",
            "d_loss:0.5495475092320703\n",
            "g_loss:[0.86568683385849, 0.8620502948760986, 0.0018182771746069193]\n",
            "Batch:501\n",
            "d_loss:0.5453526771291308\n",
            "g_loss:[0.7404320240020752, 0.7372879981994629, 0.0015719980001449585]\n",
            "========================================\n",
            "Epoch is: 6\n",
            "Number of batches 505\n",
            "Batch:1\n",
            "d_loss:0.5585939711988885\n",
            "g_loss:[0.7842752933502197, 0.7809327840805054, 0.0016712475335225463]\n",
            "Batch:101\n",
            "d_loss:0.5491399807128801\n",
            "g_loss:[0.7103152871131897, 0.7070950865745544, 0.0016101024812087417]\n",
            "Batch:201\n",
            "d_loss:0.5575304427222818\n",
            "g_loss:[0.8427973985671997, 0.8397910594940186, 0.0015031653456389904]\n",
            "Batch:301\n",
            "d_loss:0.555616124106109\n",
            "g_loss:[0.7532885074615479, 0.7504613995552063, 0.001413546153344214]\n",
            "Batch:401\n",
            "d_loss:0.5450420979941555\n",
            "g_loss:[0.741018533706665, 0.7383104562759399, 0.0013540489599108696]\n",
            "Batch:501\n",
            "d_loss:0.5444972266075183\n",
            "g_loss:[0.7406499981880188, 0.7383254766464233, 0.0011622612364590168]\n",
            "========================================\n",
            "Epoch is: 7\n",
            "Number of batches 505\n",
            "Batch:1\n",
            "d_loss:0.5511105584530469\n",
            "g_loss:[0.7490310072898865, 0.7465159893035889, 0.0012575073633342981]\n",
            "Batch:101\n",
            "d_loss:0.5490767108567525\n",
            "g_loss:[0.7597277164459229, 0.757335901260376, 0.0011959108524024487]\n",
            "Batch:201\n",
            "d_loss:0.5544003008467371\n",
            "g_loss:[0.7845916152000427, 0.7823256254196167, 0.0011329887202009559]\n",
            "Batch:301\n",
            "d_loss:0.5400886927673128\n",
            "g_loss:[0.7996143698692322, 0.7974737882614136, 0.0010702975559979677]\n",
            "Batch:401\n",
            "d_loss:0.5441680325529887\n",
            "g_loss:[0.8037410974502563, 0.8017207384109497, 0.001010168227367103]\n",
            "Batch:501\n",
            "d_loss:0.5421813773259601\n",
            "g_loss:[0.8061097264289856, 0.8043102025985718, 0.0008997626719065011]\n",
            "========================================\n",
            "Epoch is: 8\n",
            "Number of batches 505\n",
            "Batch:1\n",
            "d_loss:0.5458657937688258\n",
            "g_loss:[0.8135079741477966, 0.8115329146385193, 0.000987516832537949]\n",
            "Batch:101\n",
            "d_loss:0.5432557321851164\n",
            "g_loss:[0.8033114671707153, 0.8014377355575562, 0.0009368777391500771]\n",
            "Batch:201\n",
            "d_loss:0.5740674049597487\n",
            "g_loss:[0.7787260413169861, 0.7769781947135925, 0.0008739272016100585]\n",
            "Batch:301\n",
            "d_loss:0.5512831637203135\n",
            "g_loss:[0.7749640345573425, 0.773280143737793, 0.0008419560035690665]\n",
            "Batch:401\n",
            "d_loss:0.5498794698360143\n",
            "g_loss:[0.7894108891487122, 0.7878327369689941, 0.0007890824927017093]\n",
            "Batch:501\n",
            "d_loss:0.5422969644474733\n",
            "g_loss:[0.7802274823188782, 0.7788882851600647, 0.0006696005002595484]\n",
            "========================================\n",
            "Epoch is: 9\n",
            "Number of batches 505\n",
            "Batch:1\n",
            "d_loss:0.5432680305175381\n",
            "g_loss:[0.7529909610748291, 0.7515760660171509, 0.0007074536406435072]\n",
            "Batch:101\n",
            "d_loss:0.5474142867687988\n",
            "g_loss:[0.7734771370887756, 0.7714998126029968, 0.0009886634070426226]\n",
            "Batch:201\n",
            "d_loss:0.5449066776839118\n",
            "g_loss:[0.7833993434906006, 0.7819850444793701, 0.0007071475265547633]\n",
            "Batch:301\n",
            "d_loss:0.5458323021896376\n",
            "g_loss:[0.8288554549217224, 0.8275554776191711, 0.000650001282338053]\n",
            "Batch:401\n",
            "d_loss:0.546308649238398\n",
            "g_loss:[0.7839862108230591, 0.7825345993041992, 0.0007258206605911255]\n",
            "Batch:501\n",
            "d_loss:0.5422554812396925\n",
            "g_loss:[0.7956984639167786, 0.7945424318313599, 0.0005780281499028206]\n",
            "========================================\n",
            "Epoch is: 10\n",
            "Number of batches 505\n",
            "Batch:1\n",
            "d_loss:0.5410360204989786\n",
            "g_loss:[0.7679688930511475, 0.7668333053588867, 0.0005677967565134168]\n",
            "Batch:101\n",
            "d_loss:0.5454504843810923\n",
            "g_loss:[0.7593705058097839, 0.7544839978218079, 0.002443266799673438]\n",
            "Batch:201\n",
            "d_loss:0.5449931276981488\n",
            "g_loss:[0.7973849177360535, 0.7959206700325012, 0.0007321155862882733]\n",
            "Batch:301\n",
            "d_loss:0.5421040069800256\n",
            "g_loss:[0.7788358926773071, 0.7773147821426392, 0.0007605600403621793]\n",
            "Batch:401\n",
            "d_loss:0.5452177582969853\n",
            "g_loss:[0.7656819820404053, 0.7634725570678711, 0.0011047117877751589]\n",
            "Batch:501\n",
            "d_loss:0.5408248177395762\n",
            "g_loss:[0.8152905702590942, 0.8126958012580872, 0.0012973841512575746]\n",
            "========================================\n",
            "Epoch is: 11\n",
            "Number of batches 505\n",
            "Batch:1\n",
            "d_loss:0.5399641692729347\n",
            "g_loss:[0.7657539248466492, 0.7645331621170044, 0.0006103813066147268]\n",
            "Batch:101\n",
            "d_loss:0.5447485067656999\n",
            "g_loss:[0.7727150917053223, 0.771560549736023, 0.000577285885810852]\n",
            "Batch:201\n",
            "d_loss:0.5441463985735027\n",
            "g_loss:[0.7970166206359863, 0.7943063974380493, 0.0013551032170653343]\n",
            "Batch:301\n",
            "d_loss:0.5403703451272577\n",
            "g_loss:[0.7858622074127197, 0.7832739949226379, 0.0012941107852384448]\n",
            "Batch:401\n",
            "d_loss:0.5448439314623101\n",
            "g_loss:[0.77730393409729, 0.7763069868087769, 0.000498483597766608]\n",
            "Batch:501\n",
            "d_loss:0.5400047954349247\n",
            "g_loss:[0.7980872392654419, 0.7972042560577393, 0.00044149113819003105]\n",
            "========================================\n",
            "Epoch is: 12\n",
            "Number of batches 505\n",
            "Batch:1\n",
            "d_loss:0.5394650162334074\n",
            "g_loss:[0.7483395338058472, 0.7469190359115601, 0.0007102634990587831]\n",
            "Batch:101\n",
            "d_loss:0.5426929815876065\n",
            "g_loss:[0.7501598000526428, 0.7488453388214111, 0.0006572236306965351]\n",
            "Batch:201\n",
            "d_loss:0.5434910045191828\n",
            "g_loss:[0.7948042750358582, 0.793426513671875, 0.00068887002998963]\n",
            "Batch:301\n",
            "d_loss:0.5393589383522794\n",
            "g_loss:[0.7962523102760315, 0.7942346334457397, 0.0010088349226862192]\n",
            "Batch:401\n",
            "d_loss:0.5445769873928157\n",
            "g_loss:[0.7694080471992493, 0.7677284479141235, 0.0008398001082241535]\n",
            "Batch:501\n",
            "d_loss:0.5387326059878887\n",
            "g_loss:[0.8090519905090332, 0.805813193321228, 0.001619399874471128]\n",
            "========================================\n",
            "Epoch is: 13\n",
            "Number of batches 505\n",
            "Batch:1\n",
            "d_loss:0.5392126822860632\n",
            "g_loss:[0.7579089999198914, 0.7545663118362427, 0.0016713484656065702]\n",
            "Batch:101\n",
            "d_loss:0.5398148496283284\n",
            "g_loss:[0.7363471984863281, 0.7348558902740479, 0.0007456475286744535]\n",
            "Batch:201\n",
            "d_loss:0.5426816183685332\n",
            "g_loss:[0.7881247997283936, 0.7872334122657776, 0.0004457028117030859]\n",
            "Batch:301\n",
            "d_loss:0.5379944793000959\n",
            "g_loss:[0.7967277765274048, 0.793367862701416, 0.00167995342053473]\n",
            "Batch:401\n",
            "d_loss:0.5440416412980085\n",
            "g_loss:[0.7820146679878235, 0.7805465459823608, 0.0007340512820519507]\n",
            "Batch:501\n",
            "d_loss:0.5381964901498577\n",
            "g_loss:[0.7914435863494873, 0.7906017303466797, 0.00042092372314073145]\n",
            "========================================\n",
            "Epoch is: 14\n",
            "Number of batches 505\n",
            "Batch:1\n",
            "d_loss:0.538788377077708\n",
            "g_loss:[0.7737127542495728, 0.7721805572509766, 0.0007660994306206703]\n",
            "Batch:101\n",
            "d_loss:0.5376789437176512\n",
            "g_loss:[0.7591850757598877, 0.7585189342498779, 0.00033305783290416]\n",
            "Batch:201\n",
            "d_loss:0.5413539938197118\n",
            "g_loss:[0.7856646776199341, 0.7846707105636597, 0.0004969836445525289]\n",
            "Batch:301\n",
            "d_loss:0.5461913952618715\n",
            "g_loss:[1.095532774925232, 1.0944368839263916, 0.0005479496903717518]\n",
            "Batch:401\n",
            "d_loss:0.5433352659458706\n",
            "g_loss:[0.7946521043777466, 0.7939430475234985, 0.00035452452721074224]\n",
            "Batch:501\n",
            "d_loss:0.5348522290996698\n",
            "g_loss:[0.7788745164871216, 0.777330756187439, 0.000771878520026803]\n",
            "========================================\n",
            "Epoch is: 15\n",
            "Number of batches 505\n",
            "Batch:1\n",
            "d_loss:0.5399292622323628\n",
            "g_loss:[0.7589249610900879, 0.756811261177063, 0.0010568357538431883]\n",
            "Batch:101\n",
            "d_loss:0.537010226905295\n",
            "g_loss:[0.7733190059661865, 0.7677263021469116, 0.0027963651809841394]\n",
            "Batch:201\n",
            "d_loss:0.5408441503540757\n",
            "g_loss:[0.7804831266403198, 0.7795159220695496, 0.0004835994332097471]\n",
            "Batch:301\n",
            "d_loss:0.5321379038168743\n",
            "g_loss:[0.8282273411750793, 0.8275789022445679, 0.0003242337261326611]\n",
            "Batch:401\n",
            "d_loss:0.5420126874432754\n",
            "g_loss:[0.7800763249397278, 0.7794053554534912, 0.00033547531347721815]\n",
            "Batch:501\n",
            "d_loss:0.5350415492732736\n",
            "g_loss:[0.788015604019165, 0.7847334146499634, 0.0016411027172580361]\n",
            "========================================\n",
            "Epoch is: 16\n",
            "Number of batches 505\n",
            "Batch:1\n",
            "d_loss:0.5380612889025542\n",
            "g_loss:[0.7791322469711304, 0.7778826951980591, 0.0006247627316042781]\n",
            "Batch:101\n",
            "d_loss:0.5346875510781501\n",
            "g_loss:[0.7750816941261292, 0.7707311511039734, 0.002175278263166547]\n",
            "Batch:201\n",
            "d_loss:0.5405418225564347\n",
            "g_loss:[0.7941156625747681, 0.7916299104690552, 0.0012428616173565388]\n",
            "Batch:301\n",
            "d_loss:0.5298711913638385\n",
            "g_loss:[0.810614824295044, 0.8000059127807617, 0.0053044455125927925]\n",
            "Batch:401\n",
            "d_loss:0.5404550498354297\n",
            "g_loss:[0.7947288751602173, 0.7863242030143738, 0.004202340729534626]\n",
            "Batch:501\n",
            "d_loss:0.534993587062047\n",
            "g_loss:[0.785442054271698, 0.7842212319374084, 0.0006104101776145399]\n",
            "========================================\n",
            "Epoch is: 17\n",
            "Number of batches 505\n",
            "Batch:1\n",
            "d_loss:0.5377250453460647\n",
            "g_loss:[0.7790408134460449, 0.77787184715271, 0.0005844727857038379]\n",
            "Batch:101\n",
            "d_loss:0.5332291526732433\n",
            "g_loss:[0.7771208882331848, 0.7763097882270813, 0.0004055574827361852]\n",
            "Batch:201\n",
            "d_loss:0.539844196543072\n",
            "g_loss:[0.7838646769523621, 0.7823265194892883, 0.0007690683705732226]\n",
            "Batch:301\n",
            "d_loss:0.5288585480429902\n",
            "g_loss:[0.8693370223045349, 0.8671920299530029, 0.001072498969733715]\n",
            "Batch:401\n",
            "d_loss:0.5409268346666067\n",
            "g_loss:[0.784255862236023, 0.7833686470985413, 0.00044362113112583756]\n",
            "Batch:501\n",
            "d_loss:0.5349607113969341\n",
            "g_loss:[0.7839820981025696, 0.7832591533660889, 0.0003614585439208895]\n",
            "========================================\n",
            "Epoch is: 18\n",
            "Number of batches 505\n",
            "Batch:1\n",
            "d_loss:0.5374899977057339\n",
            "g_loss:[0.7808888554573059, 0.7804208993911743, 0.0002339807542739436]\n",
            "Batch:101\n",
            "d_loss:0.5320533523577069\n",
            "g_loss:[0.7881246209144592, 0.7860965132713318, 0.0010140574304386973]\n",
            "Batch:201\n",
            "d_loss:0.5394086253954811\n",
            "g_loss:[0.7764105796813965, 0.7759284973144531, 0.0002410531888017431]\n",
            "Batch:301\n",
            "d_loss:0.5283901952598171\n",
            "g_loss:[0.8160827159881592, 0.8120136260986328, 0.0020345477387309074]\n",
            "Batch:401\n",
            "d_loss:0.539155621098871\n",
            "g_loss:[0.7594740986824036, 0.7589770555496216, 0.0002485149016138166]\n",
            "Batch:501\n",
            "d_loss:0.5343871854749978\n",
            "g_loss:[0.8201726078987122, 0.8161901235580444, 0.0019912454299628735]\n",
            "========================================\n",
            "Epoch is: 19\n",
            "Number of batches 505\n",
            "Batch:1\n",
            "d_loss:0.5366896539965182\n",
            "g_loss:[0.7783851027488708, 0.7744666934013367, 0.001959211193025112]\n",
            "Batch:101\n",
            "d_loss:0.5313518811326503\n",
            "g_loss:[0.7342721819877625, 0.7337799072265625, 0.00024614541325718164]\n",
            "Batch:201\n",
            "d_loss:0.5393550389758275\n",
            "g_loss:[0.6338741779327393, 0.6256572008132935, 0.0041084736585617065]\n",
            "Batch:301\n",
            "d_loss:0.526543135933025\n",
            "g_loss:[0.5199952721595764, 0.5178133249282837, 0.0010909750126302242]\n",
            "Batch:401\n",
            "d_loss:0.5378882796430844\n",
            "g_loss:[0.46410050988197327, 0.46363019943237305, 0.0002351612492930144]\n",
            "Batch:501\n",
            "d_loss:0.5337549243668036\n",
            "g_loss:[0.4444340467453003, 0.4425863027572632, 0.000923867104575038]\n",
            "========================================\n",
            "Epoch is: 20\n",
            "Number of batches 505\n",
            "Batch:1\n",
            "d_loss:0.536271278815093\n",
            "g_loss:[0.45489245653152466, 0.4524015784263611, 0.0012454432435333729]\n",
            "Batch:101\n",
            "d_loss:0.5308417203077624\n",
            "g_loss:[0.3802998960018158, 0.3760209381580353, 0.00213948218151927]\n",
            "Batch:201\n",
            "d_loss:0.5390595200155985\n",
            "g_loss:[0.415189266204834, 0.4131356179714203, 0.0010268290061503649]\n",
            "Batch:301\n",
            "d_loss:0.524465644117754\n",
            "g_loss:[0.45250919461250305, 0.4478950500488281, 0.002307069953531027]\n",
            "Batch:401\n",
            "d_loss:0.5354641124374666\n",
            "g_loss:[0.4282873570919037, 0.4279053211212158, 0.00019101914949715137]\n",
            "Batch:501\n",
            "d_loss:0.5330226305061387\n",
            "g_loss:[0.4645591378211975, 0.463651180267334, 0.0004539749352261424]\n",
            "========================================\n",
            "Epoch is: 21\n",
            "Number of batches 505\n",
            "Batch:1\n",
            "d_loss:0.5356557974705538\n",
            "g_loss:[0.4849902093410492, 0.48441174626350403, 0.00028923447825945914]\n",
            "Batch:101\n",
            "d_loss:0.5303758669376748\n",
            "g_loss:[0.4086662232875824, 0.40756502747535706, 0.0005505980225279927]\n",
            "Batch:201\n",
            "d_loss:0.5382981349594047\n",
            "g_loss:[0.4374732971191406, 0.4319841265678406, 0.002744584111496806]\n",
            "Batch:301\n",
            "d_loss:0.5233821213205374\n",
            "g_loss:[0.44907069206237793, 0.4481339752674103, 0.00046836124965921044]\n",
            "Batch:401\n",
            "d_loss:0.5342032322132582\n",
            "g_loss:[0.4033936858177185, 0.40278351306915283, 0.0003050935920327902]\n",
            "Batch:501\n",
            "d_loss:0.5321340081754897\n",
            "g_loss:[0.4437443017959595, 0.44290798902511597, 0.0004181531840004027]\n",
            "========================================\n",
            "Epoch is: 22\n",
            "Number of batches 505\n",
            "Batch:1\n",
            "d_loss:0.5348077386433943\n",
            "g_loss:[0.4551764726638794, 0.4532058835029602, 0.000985296443104744]\n",
            "Batch:101\n",
            "d_loss:0.5299320747421916\n",
            "g_loss:[0.42414090037345886, 0.42376068234443665, 0.00019011083350051194]\n",
            "Batch:201\n",
            "d_loss:0.5374578077335173\n",
            "g_loss:[0.4331514537334442, 0.4320325255393982, 0.0005594646208919585]\n",
            "Batch:301\n",
            "d_loss:0.5219569530581794\n",
            "g_loss:[0.4231261909008026, 0.4163843095302582, 0.0033709360286593437]\n",
            "Batch:401\n",
            "d_loss:0.5328586328250822\n",
            "g_loss:[0.39711183309555054, 0.392068088054657, 0.0025218664668500423]\n",
            "Batch:501\n",
            "d_loss:0.5317463735650563\n",
            "g_loss:[0.42106685042381287, 0.4202771484851837, 0.00039484925218857825]\n",
            "========================================\n",
            "Epoch is: 23\n",
            "Number of batches 505\n",
            "Batch:1\n",
            "d_loss:0.5338043254332661\n",
            "g_loss:[0.43380364775657654, 0.4321502447128296, 0.0008266987861134112]\n",
            "Batch:101\n",
            "d_loss:0.5296285057283967\n",
            "g_loss:[0.4109340012073517, 0.40972521901130676, 0.0006043872563168406]\n",
            "Batch:201\n",
            "d_loss:0.5369747069885307\n",
            "g_loss:[0.40645161271095276, 0.4061313271522522, 0.0001601415715413168]\n",
            "Batch:301\n",
            "d_loss:0.5212668707590637\n",
            "g_loss:[0.40916362404823303, 0.40831267833709717, 0.00042547122575342655]\n",
            "Batch:401\n",
            "d_loss:0.5322991012471903\n",
            "g_loss:[0.39631757140159607, 0.39415493607521057, 0.0010813125409185886]\n",
            "Batch:501\n",
            "d_loss:0.5313964091224079\n",
            "g_loss:[0.4037838876247406, 0.4022235572338104, 0.0007801714818924665]\n",
            "========================================\n",
            "Epoch is: 24\n",
            "Number of batches 505\n",
            "Batch:1\n",
            "d_loss:0.5333542391445008\n",
            "g_loss:[0.4176997244358063, 0.41653427481651306, 0.0005827262066304684]\n",
            "Batch:101\n",
            "d_loss:0.5292305369371206\n",
            "g_loss:[0.4197753071784973, 0.41516345739364624, 0.002305921632796526]\n",
            "Batch:201\n",
            "d_loss:0.5364031014523789\n",
            "g_loss:[0.40765342116355896, 0.40611350536346436, 0.0007699526031501591]\n",
            "Batch:301\n",
            "d_loss:0.5196716178588758\n",
            "g_loss:[0.39599117636680603, 0.3946501612663269, 0.0006705112173222005]\n",
            "Batch:401\n",
            "d_loss:0.5306363660579336\n",
            "g_loss:[0.3736858665943146, 0.37204134464263916, 0.0008222575997933745]\n",
            "Batch:501\n",
            "d_loss:0.5315172777782209\n",
            "g_loss:[0.3963468670845032, 0.3933671712875366, 0.0014898409135639668]\n",
            "========================================\n",
            "Epoch is: 25\n",
            "Number of batches 505\n",
            "Batch:1\n",
            "d_loss:0.5325153817566388\n",
            "g_loss:[0.40772268176078796, 0.40640246868133545, 0.0006601065397262573]\n",
            "Batch:101\n",
            "d_loss:0.5292405011605297\n",
            "g_loss:[0.40308111906051636, 0.4022190570831299, 0.0004310313961468637]\n",
            "Batch:201\n",
            "d_loss:0.5360972267239958\n",
            "g_loss:[0.3895576000213623, 0.3871522545814514, 0.0012026717886328697]\n",
            "Batch:301\n",
            "d_loss:0.5194654258239098\n",
            "g_loss:[0.3764389455318451, 0.3750893473625183, 0.0006748046725988388]\n",
            "Batch:401\n",
            "d_loss:0.5299467883392026\n",
            "g_loss:[0.356020450592041, 0.3527138829231262, 0.0016532806912437081]\n",
            "Batch:501\n",
            "d_loss:0.5313539777343976\n",
            "g_loss:[0.3764306902885437, 0.37442949414253235, 0.0010005938820540905]\n",
            "========================================\n",
            "Epoch is: 26\n",
            "Number of batches 505\n",
            "Batch:1\n",
            "d_loss:0.5322283563182282\n",
            "g_loss:[0.3938181400299072, 0.3917367458343506, 0.0010407009394839406]\n",
            "Batch:101\n",
            "d_loss:0.5287117427544672\n",
            "g_loss:[0.3915449380874634, 0.3911924362182617, 0.00017624360043555498]\n",
            "Batch:201\n",
            "d_loss:0.5357694177828307\n",
            "g_loss:[0.3793228268623352, 0.37891098856925964, 0.00020591453358065337]\n",
            "Batch:301\n",
            "d_loss:0.5220615504367743\n",
            "g_loss:[0.6927239298820496, 0.6890579462051392, 0.0018329824088141322]\n",
            "Batch:401\n",
            "d_loss:0.5334352953714188\n",
            "g_loss:[0.6448315978050232, 0.6445375084877014, 0.00014703554916195571]\n",
            "Batch:501\n",
            "d_loss:0.5284463870020772\n",
            "g_loss:[0.583509624004364, 0.5796780586242676, 0.0019157775677740574]\n",
            "========================================\n",
            "Epoch is: 27\n",
            "Number of batches 505\n",
            "Batch:1\n",
            "d_loss:0.5320451362304084\n",
            "g_loss:[0.5810027718544006, 0.5752794742584229, 0.0028616501949727535]\n",
            "Batch:101\n",
            "d_loss:0.5264412063152122\n",
            "g_loss:[0.5372288227081299, 0.536878764629364, 0.00017502339323982596]\n",
            "Batch:201\n",
            "d_loss:0.5357116617624342\n",
            "g_loss:[0.6166441440582275, 0.6123301982879639, 0.002156977541744709]\n",
            "Batch:301\n",
            "d_loss:0.5179587860038737\n",
            "g_loss:[0.6019752025604248, 0.5964106321334839, 0.0027822742704302073]\n",
            "Batch:401\n",
            "d_loss:0.5296267384110251\n",
            "g_loss:[0.5859463810920715, 0.5806666612625122, 0.0026398743502795696]\n",
            "Batch:501\n",
            "d_loss:0.529775411785522\n",
            "g_loss:[0.698813796043396, 0.6197166442871094, 0.039548564702272415]\n",
            "========================================\n",
            "Epoch is: 28\n",
            "Number of batches 505\n",
            "Batch:1\n",
            "d_loss:0.5327092300776712\n",
            "g_loss:[0.714390754699707, 0.6408606767654419, 0.036765046417713165]\n",
            "Batch:101\n",
            "d_loss:0.5271286474853696\n",
            "g_loss:[0.6190481781959534, 0.6185253262519836, 0.0002614333061501384]\n",
            "Batch:201\n",
            "d_loss:0.534274564824841\n",
            "g_loss:[0.5994880199432373, 0.5992640256881714, 0.00011199028813280165]\n",
            "Batch:301\n",
            "d_loss:0.5166295025615\n",
            "g_loss:[0.5764265656471252, 0.5754109621047974, 0.0005077952519059181]\n",
            "Batch:401\n",
            "d_loss:0.5281134789920543\n",
            "g_loss:[0.5652607083320618, 0.5650615692138672, 9.955555287888274e-05]\n",
            "Batch:501\n",
            "d_loss:0.5302495509401979\n",
            "g_loss:[0.5806554555892944, 0.5794193148612976, 0.0006180721102282405]\n",
            "========================================\n",
            "Epoch is: 29\n",
            "Number of batches 505\n",
            "Batch:1\n",
            "d_loss:0.5309685449819881\n",
            "g_loss:[0.594051718711853, 0.5933124423027039, 0.00036962394369766116]\n",
            "Batch:101\n",
            "d_loss:0.527223395252804\n",
            "g_loss:[0.6124219298362732, 0.6121945381164551, 0.00011370158608769998]\n",
            "Batch:201\n",
            "d_loss:0.5330238151573212\n",
            "g_loss:[0.6217103004455566, 0.6215944290161133, 5.79370534978807e-05]\n",
            "Batch:301\n",
            "d_loss:0.5158923469152796\n",
            "g_loss:[0.6007564663887024, 0.5989156365394592, 0.0009204200468957424]\n",
            "Batch:401\n",
            "d_loss:0.5262187163480121\n",
            "g_loss:[0.5402541756629944, 0.5396393537521362, 0.00030741377850063145]\n",
            "Batch:501\n",
            "d_loss:0.5301603303796583\n",
            "g_loss:[0.5866073966026306, 0.5854089260101318, 0.0005992481019347906]\n",
            "========================================\n",
            "Epoch is: 30\n",
            "Number of batches 505\n",
            "Batch:1\n",
            "d_loss:0.5313186725063588\n",
            "g_loss:[0.5946048498153687, 0.5941479206085205, 0.00022845817147754133]\n",
            "Batch:101\n",
            "d_loss:0.5257116104617126\n",
            "g_loss:[0.5961124897003174, 0.5954104065895081, 0.0003510337555781007]\n",
            "Batch:201\n",
            "d_loss:0.5321762638332075\n",
            "g_loss:[0.5869292616844177, 0.5863649845123291, 0.00028213736368343234]\n",
            "Batch:301\n",
            "d_loss:0.5153482146124588\n",
            "g_loss:[0.5534698963165283, 0.5527926683425903, 0.00033860764233395457]\n",
            "Batch:401\n",
            "d_loss:0.5260394880738204\n",
            "g_loss:[0.5283393859863281, 0.5267329216003418, 0.0008032462210394442]\n",
            "Batch:501\n",
            "d_loss:0.529376034897723\n",
            "g_loss:[0.5503218173980713, 0.5497583150863647, 0.00028173794271424413]\n",
            "========================================\n",
            "Epoch is: 31\n",
            "Number of batches 505\n",
            "Batch:1\n",
            "d_loss:0.5306385899566521\n",
            "g_loss:[0.5555009841918945, 0.553734540939331, 0.0008832180174067616]\n",
            "Batch:101\n",
            "d_loss:0.5246004059245024\n",
            "g_loss:[0.5462749600410461, 0.5460543632507324, 0.00011029921006411314]\n",
            "Batch:201\n",
            "d_loss:0.5306038501903458\n",
            "g_loss:[0.5483800172805786, 0.5472655892372131, 0.0005572227528318763]\n",
            "Batch:301\n",
            "d_loss:0.5138045999274254\n",
            "g_loss:[0.49642807245254517, 0.4956093430519104, 0.00040936312871053815]\n",
            "Batch:401\n",
            "d_loss:0.5243938934072503\n",
            "g_loss:[0.49460920691490173, 0.48226410150527954, 0.006172559689730406]\n",
            "Batch:501\n",
            "d_loss:0.5287282791668986\n",
            "g_loss:[0.4936811029911041, 0.49342572689056396, 0.00012768605665769428]\n",
            "========================================\n",
            "Epoch is: 32\n",
            "Number of batches 505\n",
            "Batch:1\n",
            "d_loss:0.5296741087277042\n",
            "g_loss:[0.482747882604599, 0.48241692781448364, 0.00016547340783290565]\n",
            "Batch:101\n",
            "d_loss:0.524284631521823\n",
            "g_loss:[0.541825532913208, 0.5373566746711731, 0.0022344361059367657]\n",
            "Batch:201\n",
            "d_loss:0.5292665109354857\n",
            "g_loss:[0.5162804126739502, 0.5129140615463257, 0.0016831740504130721]\n",
            "Batch:301\n",
            "d_loss:0.5110841272180551\n",
            "g_loss:[0.4644903838634491, 0.45973730087280273, 0.0023765438236296177]\n",
            "Batch:401\n",
            "d_loss:0.523495229675973\n",
            "g_loss:[0.4410361051559448, 0.4405999481678009, 0.00021807488519698381]\n",
            "Batch:501\n",
            "d_loss:0.528162832682483\n",
            "g_loss:[0.44279050827026367, 0.44240158796310425, 0.00019446536316536367]\n",
            "========================================\n",
            "Epoch is: 33\n",
            "Number of batches 505\n",
            "Batch:1\n",
            "d_loss:0.5268492515497201\n",
            "g_loss:[0.424689382314682, 0.4242849349975586, 0.00020222074817866087]\n",
            "Batch:101\n",
            "d_loss:0.5232838495658143\n",
            "g_loss:[0.49952906370162964, 0.49516183137893677, 0.002183611039072275]\n",
            "Batch:201\n",
            "d_loss:0.5274012682939428\n",
            "g_loss:[0.4782428741455078, 0.4768914580345154, 0.0006757125956937671]\n",
            "Batch:301\n",
            "d_loss:0.5071391813926311\n",
            "g_loss:[0.4273642599582672, 0.42268410325050354, 0.0023400848731398582]\n",
            "Batch:401\n",
            "d_loss:0.5221048949060787\n",
            "g_loss:[0.4070683419704437, 0.4007299840450287, 0.003169185249134898]\n",
            "Batch:501\n",
            "d_loss:0.5251358828227239\n",
            "g_loss:[0.3999136686325073, 0.39897143840789795, 0.0004711129004135728]\n",
            "========================================\n",
            "Epoch is: 34\n",
            "Number of batches 505\n",
            "Batch:1\n",
            "d_loss:0.5232271071254218\n",
            "g_loss:[0.3850001096725464, 0.3840738534927368, 0.0004631208721548319]\n",
            "Batch:101\n",
            "d_loss:0.522097104047134\n",
            "g_loss:[0.46072784066200256, 0.4503682255744934, 0.0051798028871417046]\n",
            "Batch:201\n",
            "d_loss:0.5260831700506969\n",
            "g_loss:[0.4215959310531616, 0.42095136642456055, 0.00032228746567852795]\n",
            "Batch:301\n",
            "d_loss:0.5029921381974418\n",
            "g_loss:[0.39954468607902527, 0.38724833726882935, 0.00614817114546895]\n",
            "Batch:401\n",
            "d_loss:0.5205444015273315\n",
            "g_loss:[0.36602145433425903, 0.3657984733581543, 0.0001114833212341182]\n",
            "Batch:501\n",
            "d_loss:0.5210704983505821\n",
            "g_loss:[0.37688755989074707, 0.3767407536506653, 7.340176671277732e-05]\n",
            "========================================\n",
            "Epoch is: 35\n",
            "Number of batches 505\n",
            "Batch:1\n",
            "d_loss:0.518449888523719\n",
            "g_loss:[0.3619896471500397, 0.36187422275543213, 5.771900396212004e-05]\n",
            "Batch:101\n",
            "d_loss:0.5200044190623885\n",
            "g_loss:[0.4329361021518707, 0.43197691440582275, 0.00047959567746147513]\n",
            "Batch:201\n",
            "d_loss:0.524330400728104\n",
            "g_loss:[0.40670520067214966, 0.40477585792541504, 0.0009646712569519877]\n",
            "Batch:301\n",
            "d_loss:0.49872536760449293\n",
            "g_loss:[0.3827868103981018, 0.38165464997291565, 0.0005660797469317913]\n",
            "Batch:401\n",
            "d_loss:0.5190496616723976\n",
            "g_loss:[0.3606305718421936, 0.3601093590259552, 0.00026060722302645445]\n",
            "Batch:501\n",
            "d_loss:0.516867271191586\n",
            "g_loss:[0.37987020611763, 0.3797350525856018, 6.758069503121078e-05]\n",
            "========================================\n",
            "Epoch is: 36\n",
            "Number of batches 505\n",
            "Batch:1\n",
            "d_loss:0.5143666319800104\n",
            "g_loss:[0.3628920316696167, 0.3628002405166626, 4.588963929563761e-05]\n",
            "Batch:101\n",
            "d_loss:0.5172587980032404\n",
            "g_loss:[0.41957733035087585, 0.4192904829978943, 0.00014342510257847607]\n",
            "Batch:201\n",
            "d_loss:0.5211833771127203\n",
            "g_loss:[0.3912374973297119, 0.3910113573074341, 0.00011307066597510129]\n",
            "Batch:301\n",
            "d_loss:0.49321805326144386\n",
            "g_loss:[0.3689833879470825, 0.3685142397880554, 0.00023457047063857317]\n",
            "Batch:401\n",
            "d_loss:0.5178295743535273\n",
            "g_loss:[0.35578426718711853, 0.34911689162254333, 0.0033336894121021032]\n",
            "Batch:501\n",
            "d_loss:0.5104382988884026\n",
            "g_loss:[0.3572224974632263, 0.3569227159023285, 0.0001498842757428065]\n",
            "========================================\n",
            "Epoch is: 37\n",
            "Number of batches 505\n",
            "Batch:1\n",
            "d_loss:0.509780209429664\n",
            "g_loss:[0.3502071499824524, 0.3498987555503845, 0.00015419037663377821]\n",
            "Batch:101\n",
            "d_loss:0.5142714200437695\n",
            "g_loss:[0.3883630633354187, 0.3881484270095825, 0.00010732334339991212]\n",
            "Batch:201\n",
            "d_loss:0.5170155995547248\n",
            "g_loss:[0.3700788617134094, 0.3691869378089905, 0.00044596270890906453]\n",
            "Batch:301\n",
            "d_loss:0.4872501686859323\n",
            "g_loss:[0.35616612434387207, 0.34788334369659424, 0.004141383804380894]\n",
            "Batch:401\n",
            "d_loss:0.5164589727974089\n",
            "g_loss:[0.33816275000572205, 0.33436086773872375, 0.0019009372917935252]\n",
            "Batch:501\n",
            "d_loss:0.5050709444140011\n",
            "g_loss:[0.33794859051704407, 0.3359733819961548, 0.0009875973919406533]\n",
            "========================================\n",
            "Epoch is: 38\n",
            "Number of batches 505\n",
            "Batch:1\n",
            "d_loss:0.5037537905227509\n",
            "g_loss:[0.3348044455051422, 0.33318766951560974, 0.0008083889842964709]\n",
            "Batch:101\n",
            "d_loss:0.5110806123461771\n",
            "g_loss:[0.352100670337677, 0.35142308473587036, 0.0003387964970897883]\n",
            "Batch:201\n",
            "d_loss:0.5125347824196069\n",
            "g_loss:[0.338823139667511, 0.338238000869751, 0.0002925660810433328]\n",
            "Batch:301\n",
            "d_loss:0.48056105967771146\n",
            "g_loss:[0.33433541655540466, 0.3340429663658142, 0.00014623059541918337]\n",
            "Batch:401\n",
            "d_loss:0.5158569777104276\n",
            "g_loss:[0.32923075556755066, 0.3281422257423401, 0.0005442682886496186]\n",
            "Batch:501\n",
            "d_loss:0.4969163876166931\n",
            "g_loss:[0.33208930492401123, 0.32904937863349915, 0.0015199612826108932]\n",
            "========================================\n",
            "Epoch is: 39\n",
            "Number of batches 505\n",
            "Batch:1\n",
            "d_loss:0.4968996464886004\n",
            "g_loss:[0.33404073119163513, 0.32888999581336975, 0.002575363963842392]\n",
            "Batch:101\n",
            "d_loss:0.5074048721235158\n",
            "g_loss:[0.3439514935016632, 0.3421153426170349, 0.000918074045330286]\n",
            "Batch:201\n",
            "d_loss:0.5089081477253785\n",
            "g_loss:[0.33404484391212463, 0.33184778690338135, 0.0010985261760652065]\n",
            "Batch:301\n",
            "d_loss:0.4715518267767038\n",
            "g_loss:[0.33378511667251587, 0.33236148953437805, 0.000711808679625392]\n",
            "Batch:401\n",
            "d_loss:0.5114804384720628\n",
            "g_loss:[0.3283592462539673, 0.32809314131736755, 0.00013304669118952006]\n",
            "Batch:501\n",
            "d_loss:0.4866517571554141\n",
            "g_loss:[0.3284144997596741, 0.3280102610588074, 0.0002021248801611364]\n",
            "========================================\n",
            "Epoch is: 40\n",
            "Number of batches 505\n",
            "Batch:1\n",
            "d_loss:0.48903206182149006\n",
            "g_loss:[0.32984358072280884, 0.3293111324310303, 0.0002662223414517939]\n",
            "Batch:101\n",
            "d_loss:0.5018507708009565\n",
            "g_loss:[0.3351036012172699, 0.3327828049659729, 0.0011604039464145899]\n",
            "Batch:201\n",
            "d_loss:0.5005631761005134\n",
            "g_loss:[0.3303302526473999, 0.3286615014076233, 0.0008343775989487767]\n",
            "Batch:301\n",
            "d_loss:0.46312662460650245\n",
            "g_loss:[0.332371324300766, 0.3294473886489868, 0.001461964682675898]\n",
            "Batch:401\n",
            "d_loss:0.510389298082373\n",
            "g_loss:[0.32987159490585327, 0.32767453789711, 0.001098524546250701]\n",
            "Batch:501\n",
            "d_loss:0.4756553069782967\n",
            "g_loss:[0.3290541172027588, 0.3274202346801758, 0.0008169396314769983]\n",
            "========================================\n",
            "Epoch is: 41\n",
            "Number of batches 505\n",
            "Batch:1\n",
            "d_loss:0.48103201414051\n",
            "g_loss:[0.33105653524398804, 0.32853591442108154, 0.0012603142531588674]\n",
            "Batch:101\n",
            "d_loss:0.4956725634256145\n",
            "g_loss:[0.3288803994655609, 0.32867735624313354, 0.00010152518370887265]\n",
            "Batch:201\n",
            "d_loss:0.4943717036931048\n",
            "g_loss:[0.3289180099964142, 0.32821446657180786, 0.00035177322570234537]\n",
            "Batch:301\n",
            "d_loss:0.45014951444318285\n",
            "g_loss:[0.3293236792087555, 0.32834935188293457, 0.0004871605196967721]\n",
            "Batch:401\n",
            "d_loss:0.5081108644926644\n",
            "g_loss:[0.3314169943332672, 0.3271671533584595, 0.002124914200976491]\n",
            "Batch:501\n",
            "d_loss:0.4640930385503452\n",
            "g_loss:[0.3275927007198334, 0.3271524906158447, 0.00022010502289049327]\n",
            "========================================\n",
            "Epoch is: 42\n",
            "Number of batches 505\n",
            "Batch:1\n",
            "d_loss:0.4720501146866809\n",
            "g_loss:[0.331217497587204, 0.32824280858039856, 0.0014873462496325374]\n",
            "Batch:101\n",
            "d_loss:0.4891472656890983\n",
            "g_loss:[0.32830488681793213, 0.3280270993709564, 0.00013890060654375702]\n",
            "Batch:201\n",
            "d_loss:0.4855466015501406\n",
            "g_loss:[0.3284120559692383, 0.328227162361145, 9.245424007531255e-05]\n",
            "Batch:301\n",
            "d_loss:0.43571554143454705\n",
            "g_loss:[0.329645037651062, 0.32786327600479126, 0.0008908843155950308]\n",
            "Batch:401\n",
            "d_loss:0.506160882567201\n",
            "g_loss:[0.32939183712005615, 0.32800859212875366, 0.0006916201673448086]\n",
            "Batch:501\n",
            "d_loss:0.4526188996624114\n",
            "g_loss:[0.32789453864097595, 0.3267573118209839, 0.0005686124204657972]\n",
            "========================================\n",
            "Epoch is: 43\n",
            "Number of batches 505\n",
            "Batch:1\n",
            "d_loss:0.46221282728038204\n",
            "g_loss:[0.3290427625179291, 0.3275870978832245, 0.0007278395351022482]\n",
            "Batch:101\n",
            "d_loss:0.4813504485437079\n",
            "g_loss:[0.32714760303497314, 0.3268854022026062, 0.00013109874271322042]\n",
            "Batch:201\n",
            "d_loss:0.4771911648931564\n",
            "g_loss:[0.3285452723503113, 0.32746249437332153, 0.0005413874750956893]\n",
            "Batch:301\n",
            "d_loss:0.4220076365982095\n",
            "g_loss:[0.33114463090896606, 0.32852429151535034, 0.0013101760996505618]\n",
            "Batch:401\n",
            "d_loss:0.5040632505997564\n",
            "g_loss:[0.3332329988479614, 0.32705676555633545, 0.0030881220009177923]\n",
            "Batch:501\n",
            "d_loss:0.4415736519767961\n",
            "g_loss:[0.32743605971336365, 0.3272273540496826, 0.00010436016600579023]\n",
            "========================================\n",
            "Epoch is: 44\n",
            "Number of batches 505\n",
            "Batch:1\n",
            "d_loss:0.45005446166214824\n",
            "g_loss:[0.32863280177116394, 0.3284321129322052, 0.00010034556180471554]\n",
            "Batch:101\n",
            "d_loss:0.4740920155727508\n",
            "g_loss:[0.32707479596138, 0.32655566930770874, 0.0002595687401480973]\n",
            "Batch:201\n",
            "d_loss:0.46985705074621364\n",
            "g_loss:[0.3303512632846832, 0.32703840732574463, 0.001656430889852345]\n",
            "Batch:301\n",
            "d_loss:0.40511665518715745\n",
            "g_loss:[0.3326466679573059, 0.3304674029350281, 0.0010896336752921343]\n",
            "Batch:401\n",
            "d_loss:0.5050227437695867\n",
            "g_loss:[0.3286535143852234, 0.3276541829109192, 0.0004996723728254437]\n",
            "Batch:501\n",
            "d_loss:0.42866020457404375\n",
            "g_loss:[0.3309374153614044, 0.3285818099975586, 0.0011778012849390507]\n",
            "========================================\n",
            "Epoch is: 45\n",
            "Number of batches 505\n",
            "Batch:1\n",
            "d_loss:0.43953055579186184\n",
            "g_loss:[0.3293869197368622, 0.3275219798088074, 0.0009324706625193357]\n",
            "Batch:101\n",
            "d_loss:0.46121730856611975\n",
            "g_loss:[0.32711097598075867, 0.326507031917572, 0.0003019722644239664]\n",
            "Batch:201\n",
            "d_loss:0.4611286589715746\n",
            "g_loss:[0.3270154595375061, 0.32687973976135254, 6.785825826227665e-05]\n",
            "Batch:301\n",
            "d_loss:0.38670379716859316\n",
            "g_loss:[0.33235737681388855, 0.3316035568714142, 0.00037690834142267704]\n",
            "Batch:401\n",
            "d_loss:0.507530518079875\n",
            "g_loss:[0.33576831221580505, 0.33096855878829956, 0.002399876480922103]\n",
            "Batch:501\n",
            "d_loss:0.4148383460901641\n",
            "g_loss:[0.3314315378665924, 0.32801395654678345, 0.0017087934538722038]\n",
            "========================================\n",
            "Epoch is: 46\n",
            "Number of batches 505\n",
            "Batch:1\n",
            "d_loss:0.428309104406253\n",
            "g_loss:[0.3293122351169586, 0.32790064811706543, 0.0007057867478579283]\n",
            "Batch:101\n",
            "d_loss:0.4520795325661311\n",
            "g_loss:[0.33252638578414917, 0.3266790807247162, 0.002923658350482583]\n",
            "Batch:201\n",
            "d_loss:0.45323263217142085\n",
            "g_loss:[0.32779160141944885, 0.3276117444038391, 8.992933726403862e-05]\n",
            "Batch:301\n",
            "d_loss:0.367606521351604\n",
            "g_loss:[0.33695659041404724, 0.33572784066200256, 0.0006143802311271429]\n",
            "Batch:401\n",
            "d_loss:0.5072211583612045\n",
            "g_loss:[0.33479073643684387, 0.3345913887023926, 9.968005178961903e-05]\n",
            "Batch:501\n",
            "d_loss:0.4020292700738821\n",
            "g_loss:[0.33011797070503235, 0.3299348056316376, 9.15752025321126e-05]\n",
            "========================================\n",
            "Epoch is: 47\n",
            "Number of batches 505\n",
            "Batch:1\n",
            "d_loss:0.41594515694376355\n",
            "g_loss:[0.32809552550315857, 0.32799845933914185, 4.853022619499825e-05]\n",
            "Batch:101\n",
            "d_loss:0.43909556837772357\n",
            "g_loss:[0.33165833353996277, 0.330854594707489, 0.0004018723266199231]\n",
            "Batch:201\n",
            "d_loss:0.44171420810926065\n",
            "g_loss:[0.3299960196018219, 0.3297141492366791, 0.00014093122445046902]\n",
            "Batch:301\n",
            "d_loss:0.3492289360156633\n",
            "g_loss:[0.3397766649723053, 0.33880168199539185, 0.00048749384586699307]\n",
            "Batch:401\n",
            "d_loss:0.5094108476223482\n",
            "g_loss:[0.3317772448062897, 0.3286969065666199, 0.0015401738928630948]\n",
            "Batch:501\n",
            "d_loss:0.3886426187436882\n",
            "g_loss:[0.32958531379699707, 0.32944223284721375, 7.154675404308364e-05]\n",
            "========================================\n",
            "Epoch is: 48\n",
            "Number of batches 505\n",
            "Batch:1\n",
            "d_loss:0.40128113377591035\n",
            "g_loss:[0.3313782513141632, 0.3299099802970886, 0.0007341390009969473]\n",
            "Batch:101\n",
            "d_loss:0.4267351547969156\n",
            "g_loss:[0.32832297682762146, 0.32810744643211365, 0.00010777095303637907]\n",
            "Batch:201\n",
            "d_loss:0.4283697406608553\n",
            "g_loss:[0.3316118121147156, 0.32931193709373474, 0.001149933785200119]\n",
            "Batch:301\n",
            "d_loss:0.32928916236414807\n",
            "g_loss:[0.3404092490673065, 0.33985137939453125, 0.0002789332647807896]\n",
            "Batch:401\n",
            "d_loss:0.512272515330551\n",
            "g_loss:[0.3414013981819153, 0.3405414819717407, 0.00042996241245418787]\n",
            "Batch:501\n",
            "d_loss:0.3756974295802138\n",
            "g_loss:[0.3307868540287018, 0.33034735918045044, 0.00021974934497848153]\n",
            "========================================\n",
            "Epoch is: 49\n",
            "Number of batches 505\n",
            "Batch:1\n",
            "d_loss:0.3849099016324544\n",
            "g_loss:[0.33504214882850647, 0.3326147794723511, 0.0012136880541220307]\n",
            "Batch:101\n",
            "d_loss:0.41157456428118167\n",
            "g_loss:[0.3290257751941681, 0.32833558320999146, 0.0003450996009632945]\n",
            "Batch:201\n",
            "d_loss:0.41250401677098125\n",
            "g_loss:[0.3308376967906952, 0.33038529753685, 0.00022620486561208963]\n",
            "Batch:301\n",
            "d_loss:0.3123261562641346\n",
            "g_loss:[0.3418043851852417, 0.34009137749671936, 0.0008565030875615776]\n",
            "Batch:401\n",
            "d_loss:0.5146848549011338\n",
            "g_loss:[0.3474448621273041, 0.3452907204627991, 0.0010770643129944801]\n",
            "Batch:501\n",
            "d_loss:0.36382604025311593\n",
            "g_loss:[0.3331518769264221, 0.3295179009437561, 0.0018169875256717205]\n",
            "========================================\n",
            "Epoch is: 50\n",
            "Number of batches 505\n",
            "Batch:1\n",
            "d_loss:0.369504749625321\n",
            "g_loss:[0.3333006501197815, 0.3314304053783417, 0.0009351281332783401]\n",
            "Batch:101\n",
            "d_loss:0.4012226907580043\n",
            "g_loss:[0.3303326666355133, 0.32924115657806396, 0.0005457582883536816]\n",
            "Batch:201\n",
            "d_loss:0.397761718610127\n",
            "g_loss:[0.33991578221321106, 0.3286239206790924, 0.005645924247801304]\n",
            "Batch:301\n",
            "d_loss:0.2966748795179228\n",
            "g_loss:[0.353471040725708, 0.34727609157562256, 0.0030974699184298515]\n",
            "Batch:401\n",
            "d_loss:0.5145482343796175\n",
            "g_loss:[0.35757431387901306, 0.35470783710479736, 0.0014332343125715852]\n",
            "Batch:501\n",
            "d_loss:0.35435147831685754\n",
            "g_loss:[0.33476635813713074, 0.3299739956855774, 0.0023961819242686033]\n",
            "========================================\n",
            "Epoch is: 51\n",
            "Number of batches 505\n",
            "Batch:1\n",
            "d_loss:0.3487819803831371\n",
            "g_loss:[0.34215521812438965, 0.3417063355445862, 0.0002244475035695359]\n",
            "Batch:101\n",
            "d_loss:0.37742663953395095\n",
            "g_loss:[0.3320142924785614, 0.32892906665802, 0.0015426198951900005]\n",
            "Batch:201\n",
            "d_loss:0.38584285428078147\n",
            "g_loss:[0.3335200250148773, 0.3298056721687317, 0.001857181778177619]\n",
            "Batch:301\n",
            "d_loss:0.2811767303887791\n",
            "g_loss:[0.3414180278778076, 0.34102803468704224, 0.0001949961151694879]\n",
            "Batch:401\n",
            "d_loss:0.5198592599626863\n",
            "g_loss:[0.3516334593296051, 0.3508862555027008, 0.0003736054350156337]\n",
            "Batch:501\n",
            "d_loss:0.34491546830577136\n",
            "g_loss:[0.3351573646068573, 0.334227979183197, 0.0004646877059713006]\n",
            "========================================\n",
            "Epoch is: 52\n",
            "Number of batches 505\n",
            "Batch:1\n",
            "d_loss:0.3303539747830655\n",
            "g_loss:[0.3395850956439972, 0.3365776836872101, 0.00150370504707098]\n",
            "Batch:101\n",
            "d_loss:0.3508112407812405\n",
            "g_loss:[0.33111363649368286, 0.3310168981552124, 4.836509469896555e-05]\n",
            "Batch:201\n",
            "d_loss:0.3749605472166877\n",
            "g_loss:[0.33414924144744873, 0.33244168758392334, 0.0008537814137525856]\n",
            "Batch:301\n",
            "d_loss:0.27209291653946366\n",
            "g_loss:[0.3403206765651703, 0.33648166060447693, 0.001919503789395094]\n",
            "Batch:401\n",
            "d_loss:0.5352428848964337\n",
            "g_loss:[0.3609105050563812, 0.35452359914779663, 0.0031934508588165045]\n",
            "Batch:501\n",
            "d_loss:0.33286393955131643\n",
            "g_loss:[0.33640071749687195, 0.33519217371940613, 0.0006042715394869447]\n",
            "========================================\n",
            "Epoch is: 53\n",
            "Number of batches 505\n",
            "Batch:1\n",
            "d_loss:0.3077809088390495\n",
            "g_loss:[0.3441063463687897, 0.3413398265838623, 0.00138325453735888]\n",
            "Batch:101\n",
            "d_loss:0.3270574053956352\n",
            "g_loss:[0.33175769448280334, 0.3313778340816498, 0.0001899255730677396]\n",
            "Batch:201\n",
            "d_loss:0.358598366350634\n",
            "g_loss:[0.33359745144844055, 0.33040955662727356, 0.0015939517179504037]\n",
            "Batch:301\n",
            "d_loss:0.26089358353567604\n",
            "g_loss:[0.336785227060318, 0.3365618884563446, 0.00011166972399223596]\n",
            "Batch:401\n",
            "d_loss:0.5455964559914719\n",
            "g_loss:[0.34266018867492676, 0.3388861119747162, 0.0018870378844439983]\n",
            "Batch:501\n",
            "d_loss:0.3312970458759992\n",
            "g_loss:[0.33331429958343506, 0.33258962631225586, 0.0003623413504101336]\n",
            "========================================\n",
            "Epoch is: 54\n",
            "Number of batches 505\n",
            "Batch:1\n",
            "d_loss:0.2901504405454034\n",
            "g_loss:[0.3358256220817566, 0.333396315574646, 0.0012146553490310907]\n",
            "Batch:101\n",
            "d_loss:0.3077372058949095\n",
            "g_loss:[0.33188769221305847, 0.3316519856452942, 0.00011785658716689795]\n",
            "Batch:201\n",
            "d_loss:0.3539155003600172\n",
            "g_loss:[0.3313874304294586, 0.33112096786499023, 0.00013323132588993758]\n",
            "Batch:301\n",
            "d_loss:0.24765087571427102\n",
            "g_loss:[0.34626370668411255, 0.3452780246734619, 0.0004928475827910006]\n",
            "Batch:401\n",
            "d_loss:0.5443919606041518\n",
            "g_loss:[0.3685387969017029, 0.36825114488601685, 0.00014382245717570186]\n",
            "Batch:501\n",
            "d_loss:0.3251291313420097\n",
            "g_loss:[0.3452688157558441, 0.3418513834476471, 0.0017087144078686833]\n",
            "========================================\n",
            "Epoch is: 55\n",
            "Number of batches 505\n",
            "Batch:1\n",
            "d_loss:0.27898062537087753\n",
            "g_loss:[0.3623981177806854, 0.3598514795303345, 0.0012733138864859939]\n",
            "Batch:101\n",
            "d_loss:0.29789023008197635\n",
            "g_loss:[0.343226820230484, 0.33990806341171265, 0.001659379806369543]\n",
            "Batch:201\n",
            "d_loss:0.3336025448516011\n",
            "g_loss:[0.45750465989112854, 0.45634517073631287, 0.0005797470803372562]\n",
            "Batch:301\n",
            "d_loss:0.24668793031185032\n",
            "g_loss:[0.37428852915763855, 0.3730096220970154, 0.000639451143797487]\n",
            "Batch:401\n",
            "d_loss:0.5516723484815884\n",
            "g_loss:[0.3523923456668854, 0.35069143772125244, 0.0008504490833729506]\n",
            "Batch:501\n",
            "d_loss:0.3082249161780055\n",
            "g_loss:[0.3467135429382324, 0.3447822332382202, 0.0009656609618104994]\n",
            "========================================\n",
            "Epoch is: 56\n",
            "Number of batches 505\n",
            "Batch:1\n",
            "d_loss:0.26570706918937503\n",
            "g_loss:[0.35645490884780884, 0.35583850741386414, 0.00030820214306004345]\n",
            "Batch:101\n",
            "d_loss:0.28668594948976533\n",
            "g_loss:[0.46366915106773376, 0.4631918668746948, 0.00023863748356234282]\n",
            "Batch:201\n",
            "d_loss:0.30699104638188146\n",
            "g_loss:[0.35104572772979736, 0.3472091555595398, 0.001918280147947371]\n",
            "Batch:301\n",
            "d_loss:0.27550455578619903\n",
            "g_loss:[0.34077364206314087, 0.3391413390636444, 0.000816147425211966]\n",
            "Batch:401\n",
            "d_loss:0.6174982051466031\n",
            "g_loss:[0.35587042570114136, 0.3551766872406006, 0.000346868037013337]\n",
            "Batch:501\n",
            "d_loss:0.26952619512940146\n",
            "g_loss:[0.6220028400421143, 0.6211671233177185, 0.00041784654604271054]\n",
            "========================================\n",
            "Epoch is: 57\n",
            "Number of batches 505\n",
            "Batch:1\n",
            "d_loss:0.27210740034934133\n",
            "g_loss:[0.3450806736946106, 0.34486114978790283, 0.00010976364137604833]\n",
            "Batch:101\n",
            "d_loss:0.2615534267754356\n",
            "g_loss:[0.40614381432533264, 0.4052305519580841, 0.000456637964816764]\n",
            "Batch:201\n",
            "d_loss:0.3252021358857746\n",
            "g_loss:[0.33866772055625916, 0.33654534816741943, 0.0010611934121698141]\n",
            "Batch:301\n",
            "d_loss:0.2650234990289704\n",
            "g_loss:[0.3843006491661072, 0.3818020820617676, 0.0012492798268795013]\n",
            "Batch:401\n",
            "d_loss:0.5433068221427675\n",
            "g_loss:[0.3601236641407013, 0.3569570779800415, 0.0015832982026040554]\n",
            "Batch:501\n",
            "d_loss:0.27839937295220807\n",
            "g_loss:[0.4143317937850952, 0.4138806462287903, 0.00022557986085303128]\n",
            "========================================\n",
            "Epoch is: 58\n",
            "Number of batches 505\n",
            "Batch:1\n",
            "d_loss:0.3064708041493276\n",
            "g_loss:[0.4457983374595642, 0.44548842310905457, 0.00015496187552344054]\n",
            "Batch:101\n",
            "d_loss:0.23701689257723046\n",
            "g_loss:[0.42898738384246826, 0.4284476637840271, 0.0002698538883123547]\n",
            "Batch:201\n",
            "d_loss:0.3841162364115007\n",
            "g_loss:[0.3381820321083069, 0.33643612265586853, 0.0008729588007554412]\n",
            "Batch:301\n",
            "d_loss:0.22341062361851982\n",
            "g_loss:[0.4528982937335968, 0.448804646730423, 0.0020468304865062237]\n",
            "Batch:401\n",
            "d_loss:0.6135096940342919\n",
            "g_loss:[0.34896111488342285, 0.34777209162712097, 0.0005945106968283653]\n",
            "Batch:501\n",
            "d_loss:0.2696434235567722\n",
            "g_loss:[0.34427401423454285, 0.3439190983772278, 0.00017745161312632263]\n",
            "========================================\n",
            "Epoch is: 59\n",
            "Number of batches 505\n",
            "Batch:1\n",
            "d_loss:0.5359480701445136\n",
            "g_loss:[0.3698093295097351, 0.36789172887802124, 0.0009588070679455996]\n",
            "Batch:101\n",
            "d_loss:0.23434072985401144\n",
            "g_loss:[0.36078810691833496, 0.359859824180603, 0.0004641443956643343]\n",
            "Batch:201\n",
            "d_loss:0.25641138601713465\n",
            "g_loss:[0.34609606862068176, 0.34308239817619324, 0.0015068312641233206]\n",
            "Batch:301\n",
            "d_loss:0.20655075135175593\n",
            "g_loss:[0.3938891589641571, 0.38877010345458984, 0.0025595324113965034]\n",
            "Batch:401\n",
            "d_loss:0.5557069763945037\n",
            "g_loss:[0.37212443351745605, 0.36716899275779724, 0.00247771805152297]\n",
            "Batch:501\n",
            "d_loss:0.302878532224895\n",
            "g_loss:[0.4344121217727661, 0.43213486671447754, 0.0011386258993297815]\n",
            "========================================\n",
            "Epoch is: 60\n",
            "Number of batches 505\n",
            "Batch:1\n",
            "d_loss:0.5373068552039513\n",
            "g_loss:[0.36792588233947754, 0.36743733286857605, 0.0002442729892209172]\n",
            "Batch:101\n",
            "d_loss:0.3584265050894828\n",
            "g_loss:[0.3623206317424774, 0.36203500628471375, 0.0001428074319846928]\n",
            "Batch:201\n",
            "d_loss:0.28195726542617194\n",
            "g_loss:[0.36376944184303284, 0.3630404770374298, 0.00036448173341341317]\n",
            "Batch:301\n",
            "d_loss:0.2062652965041707\n",
            "g_loss:[0.3585885465145111, 0.3577025830745697, 0.00044298311695456505]\n",
            "Batch:401\n",
            "d_loss:0.3320080736903037\n",
            "g_loss:[0.3775831162929535, 0.3754716217517853, 0.0010557472705841064]\n",
            "Batch:501\n",
            "d_loss:0.31483499173191376\n",
            "g_loss:[0.37822625041007996, 0.3779613971710205, 0.00013241940177977085]\n",
            "========================================\n",
            "Epoch is: 61\n",
            "Number of batches 505\n",
            "Batch:1\n",
            "d_loss:0.24405488534284814\n",
            "g_loss:[0.42332541942596436, 0.4230533242225647, 0.00013604648120235652]\n",
            "Batch:101\n",
            "d_loss:0.5757975415308465\n",
            "g_loss:[0.3406333923339844, 0.3351822793483734, 0.0027255499735474586]\n",
            "Batch:201\n",
            "d_loss:0.4182405400206335\n",
            "g_loss:[0.3779444098472595, 0.3776094913482666, 0.00016746288747526705]\n",
            "Batch:301\n",
            "d_loss:0.2065108585843518\n",
            "g_loss:[0.4045034945011139, 0.4034116268157959, 0.0005459364037960768]\n",
            "Batch:401\n",
            "d_loss:0.3920878192548116\n",
            "g_loss:[0.48173001408576965, 0.48102086782455444, 0.0003545679210219532]\n",
            "Batch:501\n",
            "d_loss:0.2462616548968981\n",
            "g_loss:[0.33814284205436707, 0.33769726753234863, 0.00022278996766544878]\n",
            "========================================\n",
            "Epoch is: 62\n",
            "Number of batches 505\n",
            "Batch:1\n",
            "d_loss:0.4478494067770953\n",
            "g_loss:[0.389008492231369, 0.388685017824173, 0.0001617426605662331]\n",
            "Batch:101\n",
            "d_loss:0.3403899406744131\n",
            "g_loss:[0.35824257135391235, 0.35772716999053955, 0.00025769835337996483]\n",
            "Batch:201\n",
            "d_loss:0.3997589679493103\n",
            "g_loss:[0.3431055247783661, 0.34073251485824585, 0.0011864984408020973]\n",
            "Batch:301\n",
            "d_loss:0.3864965346438112\n",
            "g_loss:[0.3590376079082489, 0.35466358065605164, 0.0021870180498808622]\n",
            "Batch:401\n",
            "d_loss:0.44401677554378693\n",
            "g_loss:[0.3649037778377533, 0.364382266998291, 0.0002607594069559127]\n",
            "Batch:501\n",
            "d_loss:0.31749004494122346\n",
            "g_loss:[0.4830189049243927, 0.4812552034854889, 0.0008818453061394393]\n",
            "========================================\n",
            "Epoch is: 63\n",
            "Number of batches 505\n",
            "Batch:1\n",
            "d_loss:0.2547878301702724\n",
            "g_loss:[0.342741996049881, 0.34018784761428833, 0.001277067232877016]\n",
            "Batch:101\n",
            "d_loss:0.2340549439577444\n",
            "g_loss:[0.3411168158054352, 0.34022825956344604, 0.0004442760837264359]\n",
            "Batch:201\n",
            "d_loss:0.30123463590280153\n",
            "g_loss:[0.3413723111152649, 0.34014537930488586, 0.0006134652066975832]\n",
            "Batch:301\n",
            "d_loss:0.3129486570687732\n",
            "g_loss:[0.4034915268421173, 0.3976534307003021, 0.0029190420173108578]\n",
            "Batch:401\n",
            "d_loss:0.564439690868312\n",
            "g_loss:[0.35702958703041077, 0.35349759459495544, 0.0017659992445260286]\n",
            "Batch:501\n",
            "d_loss:0.28768044565367745\n",
            "g_loss:[0.5122540593147278, 0.5119086503982544, 0.0001727122435113415]\n",
            "========================================\n",
            "Epoch is: 64\n",
            "Number of batches 505\n",
            "Batch:1\n",
            "d_loss:0.20852508163363837\n",
            "g_loss:[0.34366437792778015, 0.3360998332500458, 0.0037822716403752565]\n",
            "Batch:101\n",
            "d_loss:0.24863222466410662\n",
            "g_loss:[0.34997764229774475, 0.34937578439712524, 0.00030092845554463565]\n",
            "Batch:201\n",
            "d_loss:0.2543905788625125\n",
            "g_loss:[0.35329481959342957, 0.3497662842273712, 0.0017642701277509332]\n",
            "Batch:301\n",
            "d_loss:0.3221465226379223\n",
            "g_loss:[0.3396734893321991, 0.33925139904022217, 0.00021105031191837043]\n",
            "Batch:401\n",
            "d_loss:0.48396732942637755\n",
            "g_loss:[0.342680960893631, 0.34163179993629456, 0.0005245840293355286]\n",
            "Batch:501\n",
            "d_loss:0.3070001867696419\n",
            "g_loss:[0.38207992911338806, 0.38190197944641113, 8.897902444005013e-05]\n",
            "========================================\n",
            "Epoch is: 65\n",
            "Number of batches 505\n",
            "Batch:1\n",
            "d_loss:0.22213190981346997\n",
            "g_loss:[0.34492364525794983, 0.34467047452926636, 0.0001265786704607308]\n",
            "Batch:101\n",
            "d_loss:0.21313170443318086\n",
            "g_loss:[0.3898901045322418, 0.3847324848175049, 0.0025788089260458946]\n",
            "Batch:201\n",
            "d_loss:0.309179794989177\n",
            "g_loss:[0.3568154275417328, 0.3535962700843811, 0.0016095737228170037]\n",
            "Batch:301\n",
            "d_loss:0.2607542057048704\n",
            "g_loss:[0.4745413362979889, 0.4729789197444916, 0.0007812140975147486]\n",
            "Batch:401\n",
            "d_loss:0.42935021067387424\n",
            "g_loss:[0.35349947214126587, 0.34675490856170654, 0.003372275736182928]\n",
            "Batch:501\n",
            "d_loss:0.22439455759513294\n",
            "g_loss:[0.3541264533996582, 0.34839320182800293, 0.002866621594876051]\n",
            "========================================\n",
            "Epoch is: 66\n",
            "Number of batches 505\n",
            "Batch:1\n",
            "d_loss:0.2265507106676523\n",
            "g_loss:[0.35276517271995544, 0.34668630361557007, 0.0030394382774829865]\n",
            "Batch:101\n",
            "d_loss:0.22909387010076898\n",
            "g_loss:[0.3659026324748993, 0.36239591240882874, 0.0017533655045554042]\n",
            "Batch:201\n",
            "d_loss:0.8286016503661813\n",
            "g_loss:[0.3505464792251587, 0.34689006209373474, 0.0018282091477885842]\n",
            "Batch:301\n",
            "d_loss:0.33330942011252773\n",
            "g_loss:[0.3396037220954895, 0.3340325951576233, 0.002785569056868553]\n",
            "Batch:401\n",
            "d_loss:0.5704815270582912\n",
            "g_loss:[0.4065096974372864, 0.4010429382324219, 0.0027333733160048723]\n",
            "Batch:501\n",
            "d_loss:0.267379964077918\n",
            "g_loss:[0.4103141129016876, 0.40986597537994385, 0.00022407190408557653]\n",
            "========================================\n",
            "Epoch is: 67\n",
            "Number of batches 505\n",
            "Batch:1\n",
            "d_loss:0.24569898782647215\n",
            "g_loss:[0.35399487614631653, 0.3534296154975891, 0.0002826363779604435]\n",
            "Batch:101\n",
            "d_loss:0.509676526833573\n",
            "g_loss:[0.4169248938560486, 0.4127305746078491, 0.0020971603225916624]\n",
            "Batch:201\n",
            "d_loss:0.30594640789786354\n",
            "g_loss:[0.34648680686950684, 0.34625715017318726, 0.00011482345871627331]\n",
            "Batch:301\n",
            "d_loss:0.33502382463848335\n",
            "g_loss:[0.34105804562568665, 0.33736199140548706, 0.0018480289727449417]\n",
            "Batch:401\n",
            "d_loss:0.4661494513420621\n",
            "g_loss:[0.3371666371822357, 0.336655855178833, 0.00025539391208440065]\n",
            "Batch:501\n",
            "d_loss:0.4661231749523722\n",
            "g_loss:[0.3465477526187897, 0.34596922993659973, 0.000289268558844924]\n",
            "========================================\n",
            "Epoch is: 68\n",
            "Number of batches 505\n",
            "Batch:1\n",
            "d_loss:0.2589034913128785\n",
            "g_loss:[0.35756534337997437, 0.35707154870033264, 0.00024689751444384456]\n",
            "Batch:101\n",
            "d_loss:0.1950066595388762\n",
            "g_loss:[0.3934762179851532, 0.3924524486064911, 0.0005118799163028598]\n",
            "Batch:201\n",
            "d_loss:0.24712666487175738\n",
            "g_loss:[0.41189098358154297, 0.4070376753807068, 0.002426647348329425]\n",
            "Batch:301\n",
            "d_loss:0.1790361941239098\n",
            "g_loss:[0.3359254002571106, 0.3345869183540344, 0.0006692378665320575]\n",
            "Batch:401\n",
            "d_loss:0.3702083257735467\n",
            "g_loss:[0.6799450516700745, 0.679507851600647, 0.0002186114143114537]\n",
            "Batch:501\n",
            "d_loss:0.3836389119460364\n",
            "g_loss:[0.36921030282974243, 0.36876380443573, 0.00022324224119074643]\n",
            "========================================\n",
            "Epoch is: 69\n",
            "Number of batches 505\n",
            "Batch:1\n",
            "d_loss:0.44408462893443357\n",
            "g_loss:[0.35004228353500366, 0.34975671768188477, 0.0001427900861017406]\n",
            "Batch:101\n",
            "d_loss:0.21833848951064283\n",
            "g_loss:[0.3522063195705414, 0.3517078161239624, 0.00024925285833887756]\n",
            "Batch:201\n",
            "d_loss:0.3307682817685418\n",
            "g_loss:[0.3629722595214844, 0.361991822719574, 0.0004902194486930966]\n",
            "Batch:301\n",
            "d_loss:0.2342905178785486\n",
            "g_loss:[0.33969199657440186, 0.335829496383667, 0.0019312537042424083]\n",
            "Batch:401\n",
            "d_loss:0.34403902506801387\n",
            "g_loss:[0.44194185733795166, 0.43865013122558594, 0.001645862590521574]\n",
            "Batch:501\n",
            "d_loss:0.43705378752201796\n",
            "g_loss:[0.3445206582546234, 0.33662641048431396, 0.003947123885154724]\n",
            "========================================\n",
            "Epoch is: 70\n",
            "Number of batches 505\n",
            "Batch:1\n",
            "d_loss:0.40884147723409114\n",
            "g_loss:[0.46047404408454895, 0.45878347754478455, 0.0008452860056422651]\n",
            "Batch:101\n",
            "d_loss:0.4120532027900481\n",
            "g_loss:[0.41412317752838135, 0.4126172661781311, 0.0007529615540988743]\n",
            "Batch:201\n",
            "d_loss:0.39707777719013393\n",
            "g_loss:[0.3714042603969574, 0.37052294611930847, 0.00044065003748983145]\n",
            "Batch:301\n",
            "d_loss:0.23931333898144658\n",
            "g_loss:[0.3489069640636444, 0.3443468511104584, 0.00228005344979465]\n",
            "Batch:401\n",
            "d_loss:0.33264447145484155\n",
            "g_loss:[0.33888405561447144, 0.3363516926765442, 0.0012661877553910017]\n",
            "Batch:501\n",
            "d_loss:0.21733053065509012\n",
            "g_loss:[0.4721003472805023, 0.4712907671928406, 0.0004047956899739802]\n",
            "========================================\n",
            "Epoch is: 71\n",
            "Number of batches 505\n",
            "Batch:1\n",
            "d_loss:0.4644743618555367\n",
            "g_loss:[0.37180379033088684, 0.36981290578842163, 0.000995440874248743]\n",
            "Batch:101\n",
            "d_loss:0.26720256092812633\n",
            "g_loss:[0.36698412895202637, 0.36548998951911926, 0.0007470627315342426]\n",
            "Batch:201\n",
            "d_loss:0.4447424329991918\n",
            "g_loss:[0.33539560437202454, 0.3342325687408447, 0.0005815123440697789]\n",
            "Batch:301\n",
            "d_loss:0.26343166014100916\n",
            "g_loss:[0.37674370408058167, 0.3718913197517395, 0.0024261954240500927]\n",
            "Batch:401\n",
            "d_loss:0.27843283279798925\n",
            "g_loss:[0.4001385271549225, 0.39499858021736145, 0.002569978591054678]\n",
            "Batch:501\n",
            "d_loss:0.24800462875691665\n",
            "g_loss:[0.3625166714191437, 0.3546404242515564, 0.003938121721148491]\n",
            "========================================\n",
            "Epoch is: 72\n",
            "Number of batches 505\n",
            "Batch:1\n",
            "d_loss:0.47652147740882356\n",
            "g_loss:[0.33965468406677246, 0.3373619616031647, 0.001146366586908698]\n",
            "Batch:101\n",
            "d_loss:0.27053652918584703\n",
            "g_loss:[0.4494519531726837, 0.44472581148147583, 0.002363070845603943]\n",
            "Batch:201\n",
            "d_loss:0.3338625879623578\n",
            "g_loss:[0.4030761420726776, 0.4027443528175354, 0.0001658900291658938]\n",
            "Batch:301\n",
            "d_loss:0.31245672365184873\n",
            "g_loss:[0.37230658531188965, 0.36492258310317993, 0.0036919936537742615]\n",
            "Batch:401\n",
            "d_loss:0.27931061838171445\n",
            "g_loss:[0.3526385426521301, 0.3517799973487854, 0.0004292747180443257]\n",
            "Batch:501\n",
            "d_loss:0.24339984518883284\n",
            "g_loss:[0.35754671692848206, 0.3563269376754761, 0.0006098824087530375]\n",
            "========================================\n",
            "Epoch is: 73\n",
            "Number of batches 505\n",
            "Batch:1\n",
            "d_loss:0.21327701195514237\n",
            "g_loss:[0.36808162927627563, 0.3640015721321106, 0.0020400327630341053]\n",
            "Batch:101\n",
            "d_loss:0.29272771035039113\n",
            "g_loss:[0.3863399624824524, 0.3852105140686035, 0.0005647270008921623]\n",
            "Batch:201\n",
            "d_loss:0.2839817535568727\n",
            "g_loss:[0.3430119454860687, 0.34256502985954285, 0.00022345507750287652]\n",
            "Batch:301\n",
            "d_loss:0.24992934500187403\n",
            "g_loss:[0.34696653485298157, 0.3441404402256012, 0.0014130447525531054]\n",
            "Batch:401\n",
            "d_loss:0.3278080736672564\n",
            "g_loss:[0.34459781646728516, 0.3435003459453583, 0.0005487368907779455]\n",
            "Batch:501\n",
            "d_loss:0.24214841286584488\n",
            "g_loss:[0.33978596329689026, 0.33959171175956726, 9.71256522461772e-05]\n",
            "========================================\n",
            "Epoch is: 74\n",
            "Number of batches 505\n",
            "Batch:1\n",
            "d_loss:0.2453875133924157\n",
            "g_loss:[0.35680386424064636, 0.3564549684524536, 0.00017444600234739482]\n",
            "Batch:101\n",
            "d_loss:0.2728308717687469\n",
            "g_loss:[0.3378913104534149, 0.33769023418426514, 0.00010053098958451301]\n",
            "Batch:201\n",
            "d_loss:0.4983973152056933\n",
            "g_loss:[0.3439693748950958, 0.34135597944259644, 0.001306696212850511]\n",
            "Batch:301\n",
            "d_loss:0.19417651841285988\n",
            "g_loss:[0.34181129932403564, 0.34032192826271057, 0.0007446889067068696]\n",
            "Batch:401\n",
            "d_loss:0.5364536118577234\n",
            "g_loss:[0.3918866515159607, 0.3915186822414398, 0.00018398337124381214]\n",
            "Batch:501\n",
            "d_loss:0.2915862900617867\n",
            "g_loss:[0.34111204743385315, 0.3408735692501068, 0.00011923368583666161]\n",
            "========================================\n",
            "Epoch is: 75\n",
            "Number of batches 505\n",
            "Batch:1\n",
            "d_loss:0.4412601713618187\n",
            "g_loss:[0.34439870715141296, 0.3439550995826721, 0.00022180019004736096]\n",
            "Batch:101\n",
            "d_loss:0.22020915435859933\n",
            "g_loss:[0.3621518313884735, 0.36083975434303284, 0.0006560329929925501]\n",
            "Batch:201\n",
            "d_loss:0.23344138467837183\n",
            "g_loss:[0.33976051211357117, 0.3358444571495056, 0.001958030741661787]\n",
            "Batch:301\n",
            "d_loss:0.19690085327601992\n",
            "g_loss:[0.3413342237472534, 0.33989620208740234, 0.000719016301445663]\n",
            "Batch:401\n",
            "d_loss:0.6128681835252792\n",
            "g_loss:[0.3894776999950409, 0.38831257820129395, 0.0005825607804581523]\n",
            "Batch:501\n",
            "d_loss:0.4265838581286516\n",
            "g_loss:[0.36860358715057373, 0.3670046329498291, 0.0007994754705578089]\n",
            "========================================\n",
            "Epoch is: 76\n",
            "Number of batches 505\n",
            "Batch:1\n",
            "d_loss:0.7353654133185046\n",
            "g_loss:[0.3423074781894684, 0.3416762351989746, 0.0003156143648084253]\n",
            "Batch:101\n",
            "d_loss:0.39509749094213475\n",
            "g_loss:[0.3381061553955078, 0.3371286690235138, 0.0004887364339083433]\n",
            "Batch:201\n",
            "d_loss:0.270647627839935\n",
            "g_loss:[0.3408847749233246, 0.3346153497695923, 0.003134708385914564]\n",
            "Batch:301\n",
            "d_loss:0.3970354676566785\n",
            "g_loss:[0.37129056453704834, 0.3710158169269562, 0.00013736920664086938]\n",
            "Batch:401\n",
            "d_loss:1.433206345362123\n",
            "g_loss:[0.37906143069267273, 0.3766776919364929, 0.0011918653035536408]\n",
            "Batch:501\n",
            "d_loss:0.38157327970657207\n",
            "g_loss:[0.34917542338371277, 0.34811773896217346, 0.0005288450047373772]\n",
            "========================================\n",
            "Epoch is: 77\n",
            "Number of batches 505\n",
            "Batch:1\n",
            "d_loss:0.2794034262446985\n",
            "g_loss:[0.36897140741348267, 0.3675853908061981, 0.0006930133095011115]\n",
            "Batch:101\n",
            "d_loss:0.23618378480205138\n",
            "g_loss:[0.3405855596065521, 0.3391174376010895, 0.0007340637966990471]\n",
            "Batch:201\n",
            "d_loss:0.2579724990937393\n",
            "g_loss:[0.34774500131607056, 0.3462887108325958, 0.000728145707398653]\n",
            "Batch:301\n",
            "d_loss:0.2042975490936101\n",
            "g_loss:[0.39375513792037964, 0.3909273147583008, 0.001413918100297451]\n",
            "Batch:401\n",
            "d_loss:0.4714917124083513\n",
            "g_loss:[0.3398727774620056, 0.3395516872406006, 0.0001605498546268791]\n",
            "Batch:501\n",
            "d_loss:0.33846623590943636\n",
            "g_loss:[0.3511870503425598, 0.349751740694046, 0.0007176562212407589]\n",
            "========================================\n",
            "Epoch is: 78\n",
            "Number of batches 505\n",
            "Batch:1\n",
            "d_loss:0.37256271917704\n",
            "g_loss:[0.34039077162742615, 0.339985191822052, 0.00020278888405300677]\n",
            "Batch:101\n",
            "d_loss:0.22777042749657994\n",
            "g_loss:[0.3652586042881012, 0.36510688066482544, 7.586547144455835e-05]\n",
            "Batch:201\n",
            "d_loss:0.27962293988093734\n",
            "g_loss:[0.38545873761177063, 0.3808208703994751, 0.0023189273197203875]\n",
            "Batch:301\n",
            "d_loss:0.20927863859606077\n",
            "g_loss:[0.8069421052932739, 0.8031314611434937, 0.0019053072901442647]\n",
            "Batch:401\n",
            "d_loss:0.49139688321702124\n",
            "g_loss:[0.38654065132141113, 0.3835318684577942, 0.0015043886378407478]\n",
            "Batch:501\n",
            "d_loss:0.19996493373309931\n",
            "g_loss:[0.3434049189090729, 0.34271544218063354, 0.00034473411506041884]\n",
            "========================================\n",
            "Epoch is: 79\n",
            "Number of batches 505\n",
            "Batch:1\n",
            "d_loss:0.20835622772628426\n",
            "g_loss:[0.3485661745071411, 0.3478925824165344, 0.00033679144689813256]\n",
            "Batch:101\n",
            "d_loss:0.259860965549251\n",
            "g_loss:[0.3522898554801941, 0.3510653078556061, 0.0006122804479673505]\n",
            "Batch:201\n",
            "d_loss:0.3435124541283585\n",
            "g_loss:[0.34624531865119934, 0.3430534899234772, 0.0015959142474457622]\n",
            "Batch:301\n",
            "d_loss:0.20927602417032176\n",
            "g_loss:[0.5807893872261047, 0.5800858736038208, 0.0003517495351843536]\n",
            "Batch:401\n",
            "d_loss:0.3729125565423601\n",
            "g_loss:[0.34932759404182434, 0.34891238808631897, 0.00020760178449563682]\n",
            "Batch:501\n",
            "d_loss:0.2401075610105181\n",
            "g_loss:[0.3368467092514038, 0.3356826603412628, 0.0005820273072458804]\n",
            "========================================\n",
            "Epoch is: 80\n",
            "Number of batches 505\n",
            "Batch:1\n",
            "d_loss:0.32702044700431543\n",
            "g_loss:[0.33347398042678833, 0.3323897123336792, 0.0005421391688287258]\n",
            "Batch:101\n",
            "d_loss:0.23709645857979922\n",
            "g_loss:[0.4043109118938446, 0.4040306508541107, 0.00014013180043548346]\n",
            "Batch:201\n",
            "d_loss:0.40870626746254857\n",
            "g_loss:[0.3477924168109894, 0.34690746665000916, 0.00044247566256672144]\n",
            "Batch:301\n",
            "d_loss:0.30432968857167\n",
            "g_loss:[0.3460066318511963, 0.3441700339317322, 0.000918305478990078]\n",
            "Batch:401\n",
            "d_loss:0.3266773985596956\n",
            "g_loss:[0.350321888923645, 0.34988266229629517, 0.00021961686434224248]\n",
            "Batch:501\n",
            "d_loss:0.44134167954769055\n",
            "g_loss:[0.337100088596344, 0.33638256788253784, 0.00035875733010470867]\n",
            "========================================\n",
            "Epoch is: 81\n",
            "Number of batches 505\n",
            "Batch:1\n",
            "d_loss:0.25441360207742036\n",
            "g_loss:[0.3324470818042755, 0.33195653557777405, 0.00024527881760150194]\n",
            "Batch:101\n",
            "d_loss:0.2173480499004654\n",
            "g_loss:[0.34395045042037964, 0.3432796597480774, 0.0003353939391672611]\n",
            "Batch:201\n",
            "d_loss:0.23547611825961212\n",
            "g_loss:[0.35373085737228394, 0.351864755153656, 0.0009330549510195851]\n",
            "Batch:301\n",
            "d_loss:0.30220766838601776\n",
            "g_loss:[0.35840049386024475, 0.35269230604171753, 0.0028540994971990585]\n",
            "Batch:401\n",
            "d_loss:0.38418208281836996\n",
            "g_loss:[0.3547162115573883, 0.35204043984413147, 0.001337879803031683]\n",
            "Batch:501\n",
            "d_loss:0.3272165269263496\n",
            "g_loss:[0.3456505239009857, 0.3441253900527954, 0.0007625604048371315]\n",
            "========================================\n",
            "Epoch is: 82\n",
            "Number of batches 505\n",
            "Batch:1\n",
            "d_loss:0.24105240742483147\n",
            "g_loss:[0.38039541244506836, 0.3791092038154602, 0.0006430996581912041]\n",
            "Batch:101\n",
            "d_loss:0.2117385221645236\n",
            "g_loss:[0.3934534192085266, 0.3920155167579651, 0.000718957744538784]\n",
            "Batch:201\n",
            "d_loss:0.21013850536837708\n",
            "g_loss:[0.3499051034450531, 0.34722885489463806, 0.0013381304452195764]\n",
            "Batch:301\n",
            "d_loss:0.18639076302042668\n",
            "g_loss:[0.3629736304283142, 0.35811638832092285, 0.0024286212865263224]\n",
            "Batch:401\n",
            "d_loss:0.3288123011770949\n",
            "g_loss:[0.34709444642066956, 0.341619074344635, 0.0027376902289688587]\n",
            "Batch:501\n",
            "d_loss:0.22550025930695483\n",
            "g_loss:[0.33450791239738464, 0.3335464596748352, 0.0004807265941053629]\n",
            "========================================\n",
            "Epoch is: 83\n",
            "Number of batches 505\n",
            "Batch:1\n",
            "d_loss:0.5163432336454434\n",
            "g_loss:[0.3661031424999237, 0.3651295304298401, 0.00048680376494303346]\n",
            "Batch:101\n",
            "d_loss:0.2625503417766595\n",
            "g_loss:[0.35570263862609863, 0.3554117977619171, 0.00014542657299898565]\n",
            "Batch:201\n",
            "d_loss:0.23955053437384777\n",
            "g_loss:[0.33063098788261414, 0.33020830154418945, 0.0002113364898832515]\n",
            "Batch:301\n",
            "d_loss:0.2062850564107066\n",
            "g_loss:[0.3360120356082916, 0.33539772033691406, 0.0003071613027714193]\n",
            "Batch:401\n",
            "d_loss:0.37916712762671523\n",
            "g_loss:[0.36131933331489563, 0.3566499352455139, 0.0023346939124166965]\n",
            "Batch:501\n",
            "d_loss:0.21759686980976767\n",
            "g_loss:[0.33948424458503723, 0.33907830715179443, 0.0002029616734944284]\n",
            "========================================\n",
            "Epoch is: 84\n",
            "Number of batches 505\n",
            "Batch:1\n",
            "d_loss:0.23898514837492257\n",
            "g_loss:[0.33813562989234924, 0.3371812105178833, 0.0004772148095071316]\n",
            "Batch:101\n",
            "d_loss:0.1910580126568675\n",
            "g_loss:[0.3391053378582001, 0.33879393339157104, 0.00015570310642942786]\n",
            "Batch:201\n",
            "d_loss:0.2680470375344157\n",
            "g_loss:[0.3504975438117981, 0.3456159830093384, 0.002440773881971836]\n",
            "Batch:301\n",
            "d_loss:0.19120778479191358\n",
            "g_loss:[0.34102070331573486, 0.33979567885398865, 0.0006125171785242856]\n",
            "Batch:401\n",
            "d_loss:0.3992816075206065\n",
            "g_loss:[0.369314968585968, 0.368242472410202, 0.0005362501833587885]\n",
            "Batch:501\n",
            "d_loss:0.19711454232128744\n",
            "g_loss:[0.3641263246536255, 0.3585621416568756, 0.0027820891700685024]\n",
            "========================================\n",
            "Epoch is: 85\n",
            "Number of batches 505\n",
            "Batch:1\n",
            "d_loss:0.22936467657564208\n",
            "g_loss:[0.36629173159599304, 0.36186325550079346, 0.0022142406087368727]\n",
            "Batch:101\n",
            "d_loss:0.32678680687422457\n",
            "g_loss:[0.34890496730804443, 0.3465230464935303, 0.0011909559834748507]\n",
            "Batch:201\n",
            "d_loss:0.25327170483069494\n",
            "g_loss:[0.35006508231163025, 0.34957003593444824, 0.00024752880563028157]\n",
            "Batch:301\n",
            "d_loss:0.1825002532423241\n",
            "g_loss:[0.3942968547344208, 0.39143919944763184, 0.0014288278762251139]\n",
            "Batch:401\n",
            "d_loss:1.2393836452392861\n",
            "g_loss:[0.40607234835624695, 0.3966712951660156, 0.004700532648712397]\n",
            "Batch:501\n",
            "d_loss:0.1950150935464876\n",
            "g_loss:[0.42931339144706726, 0.4261540472507477, 0.001579666044563055]\n",
            "========================================\n",
            "Epoch is: 86\n",
            "Number of batches 505\n",
            "Batch:1\n",
            "d_loss:0.2584776746589341\n",
            "g_loss:[0.3391517996788025, 0.33726024627685547, 0.0009457723936066031]\n",
            "Batch:101\n",
            "d_loss:0.2030181261652615\n",
            "g_loss:[0.34954699873924255, 0.34903138875961304, 0.00025780993746593595]\n",
            "Batch:201\n",
            "d_loss:0.5043581219069893\n",
            "g_loss:[0.33657610416412354, 0.33610114455223083, 0.00023747945670038462]\n",
            "Batch:301\n",
            "d_loss:0.1836681184609006\n",
            "g_loss:[0.4479174017906189, 0.44523459672927856, 0.0013414020650088787]\n",
            "Batch:401\n",
            "d_loss:0.8050409525167197\n",
            "g_loss:[0.39807286858558655, 0.3975224792957306, 0.00027519138529896736]\n",
            "Batch:501\n",
            "d_loss:0.19935781032654631\n",
            "g_loss:[0.4007605016231537, 0.3972717821598053, 0.0017443528631702065]\n",
            "========================================\n",
            "Epoch is: 87\n",
            "Number of batches 505\n",
            "Batch:1\n",
            "d_loss:0.36229208065203977\n",
            "g_loss:[0.3987899124622345, 0.396352618932724, 0.0012186423409730196]\n",
            "Batch:101\n",
            "d_loss:0.18583004664833425\n",
            "g_loss:[0.33643603324890137, 0.33470064401626587, 0.000867698690854013]\n",
            "Batch:201\n",
            "d_loss:0.23001554859820317\n",
            "g_loss:[0.34969261288642883, 0.34633898735046387, 0.0016768134664744139]\n",
            "Batch:301\n",
            "d_loss:0.20819317460700404\n",
            "g_loss:[0.357008695602417, 0.3527449071407318, 0.0021318967919796705]\n",
            "Batch:401\n",
            "d_loss:0.4404072119468765\n",
            "g_loss:[0.3459785580635071, 0.3422924876213074, 0.001843028119765222]\n",
            "Batch:501\n",
            "d_loss:0.5697359415053143\n",
            "g_loss:[0.37131595611572266, 0.363971084356308, 0.0036724358797073364]\n",
            "========================================\n",
            "Epoch is: 88\n",
            "Number of batches 505\n",
            "Batch:1\n",
            "d_loss:0.2409230925841257\n",
            "g_loss:[0.3454185724258423, 0.34416013956069946, 0.0006292143953032792]\n",
            "Batch:101\n",
            "d_loss:0.21137611780159204\n",
            "g_loss:[0.3363879323005676, 0.33403337001800537, 0.001177274389192462]\n",
            "Batch:201\n",
            "d_loss:0.2545046419254504\n",
            "g_loss:[0.3357406556606293, 0.33306533098220825, 0.001337667228654027]\n",
            "Batch:301\n",
            "d_loss:0.18068201816706164\n",
            "g_loss:[0.3811529576778412, 0.37641841173171997, 0.0023672794923186302]\n",
            "Batch:401\n",
            "d_loss:0.25748468593519647\n",
            "g_loss:[0.4557785391807556, 0.4505434036254883, 0.00261756032705307]\n",
            "Batch:501\n",
            "d_loss:0.20902938296467255\n",
            "g_loss:[0.3692152500152588, 0.3689604699611664, 0.00012738908117171377]\n",
            "========================================\n",
            "Epoch is: 89\n",
            "Number of batches 505\n",
            "Batch:1\n",
            "d_loss:0.2433457353290578\n",
            "g_loss:[0.3374386429786682, 0.33703023195266724, 0.00020420481450855732]\n",
            "Batch:101\n",
            "d_loss:0.30032076983115985\n",
            "g_loss:[0.3367030620574951, 0.33562085032463074, 0.0005411074962466955]\n",
            "Batch:201\n",
            "d_loss:0.262599532288732\n",
            "g_loss:[0.42811039090156555, 0.42557525634765625, 0.001267563784494996]\n",
            "Batch:301\n",
            "d_loss:0.19123522474455967\n",
            "g_loss:[0.36927270889282227, 0.36531150341033936, 0.001980595290660858]\n",
            "Batch:401\n",
            "d_loss:0.2995947680901736\n",
            "g_loss:[0.3613300919532776, 0.3609965741634369, 0.00016675643564667553]\n",
            "Batch:501\n",
            "d_loss:0.23560680992704874\n",
            "g_loss:[0.34156274795532227, 0.34103718400001526, 0.0002627806970849633]\n",
            "========================================\n",
            "Epoch is: 90\n",
            "Number of batches 505\n",
            "Batch:1\n",
            "d_loss:0.23300051504315888\n",
            "g_loss:[0.36749643087387085, 0.36683669686317444, 0.0003298739902675152]\n",
            "Batch:101\n",
            "d_loss:0.21795821745035937\n",
            "g_loss:[0.35533034801483154, 0.35513240098953247, 9.897608106257394e-05]\n",
            "Batch:201\n",
            "d_loss:0.22836051534977742\n",
            "g_loss:[0.3393732011318207, 0.33618685603141785, 0.0015931768575683236]\n",
            "Batch:301\n",
            "d_loss:0.1781003430953234\n",
            "g_loss:[0.34276658296585083, 0.342496395111084, 0.00013509386917576194]\n",
            "Batch:401\n",
            "d_loss:0.26109551031549927\n",
            "g_loss:[0.37200209498405457, 0.37033554911613464, 0.0008332750294357538]\n",
            "Batch:501\n",
            "d_loss:0.5370148661604617\n",
            "g_loss:[0.3396055996417999, 0.338440865278244, 0.0005823633982799947]\n",
            "========================================\n",
            "Epoch is: 91\n",
            "Number of batches 505\n",
            "Batch:1\n",
            "d_loss:0.7778395768432347\n",
            "g_loss:[0.35770225524902344, 0.35623520612716675, 0.0007335302652791142]\n",
            "Batch:101\n",
            "d_loss:0.18972122981176653\n",
            "g_loss:[0.653375506401062, 0.6520267724990845, 0.0006743581034243107]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YaBFrKPEHFK9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def save_rgb_img(img, path):\n",
        "    \"\"\"\n",
        "    Save a rgb image\n",
        "    \"\"\"\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_subplot(1, 1, 1)\n",
        "    ax.imshow(img)\n",
        "    ax.axis(\"off\")\n",
        "    ax.set_title(\"Image\")\n",
        "\n",
        "    plt.savefig(path)\n",
        "    plt.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPdYmYpG64Hk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "outputId": "2213ba6d-f189-43a9-8b92-2ca91afbcba2"
      },
      "source": [
        "z_noise2 = np.random.normal(0, 1, size=(batch_size, z_dim))\n",
        "embedding_batch = emb_test[0:batch_size]\n",
        "fake_images, _ = stage1_gen.predict_on_batch([embedding_batch, z_noise2])\n",
        "\n",
        "# Save images\n",
        "for i, img in enumerate(fake_images[:10]):\n",
        "  save_rgb_img(img, \"/content/drive/My Drive/fake_img/gen_{}_{}.png\".format(epoch, i))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DR479_u7HIUq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stage1_gen.save_weights(\"stage1_gen.h5\")\n",
        "stage1_dis.save_weights(\"stage1_dis.h5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lFzrxVP9dVV_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "outputId": "1bdbc8d0-def8-46fc-8d33-7bdb5a1c71e9"
      },
      "source": [
        "im.show(fake_images)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-41cd9afb895d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfake_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'im' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xMwuQ6idWDP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 422
        },
        "outputId": "aec62453-0366-4055-9611-fcdc00290a34"
      },
      "source": [
        "emb_test"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>984</th>\n",
              "      <th>985</th>\n",
              "      <th>986</th>\n",
              "      <th>987</th>\n",
              "      <th>988</th>\n",
              "      <th>989</th>\n",
              "      <th>990</th>\n",
              "      <th>991</th>\n",
              "      <th>992</th>\n",
              "      <th>993</th>\n",
              "      <th>994</th>\n",
              "      <th>995</th>\n",
              "      <th>996</th>\n",
              "      <th>997</th>\n",
              "      <th>998</th>\n",
              "      <th>999</th>\n",
              "      <th>1000</th>\n",
              "      <th>1001</th>\n",
              "      <th>1002</th>\n",
              "      <th>1003</th>\n",
              "      <th>1004</th>\n",
              "      <th>1005</th>\n",
              "      <th>1006</th>\n",
              "      <th>1007</th>\n",
              "      <th>1008</th>\n",
              "      <th>1009</th>\n",
              "      <th>1010</th>\n",
              "      <th>1011</th>\n",
              "      <th>1012</th>\n",
              "      <th>1013</th>\n",
              "      <th>1014</th>\n",
              "      <th>1015</th>\n",
              "      <th>1016</th>\n",
              "      <th>1017</th>\n",
              "      <th>1018</th>\n",
              "      <th>1019</th>\n",
              "      <th>1020</th>\n",
              "      <th>1021</th>\n",
              "      <th>1022</th>\n",
              "      <th>1023</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.021993</td>\n",
              "      <td>-0.333375</td>\n",
              "      <td>1.142767</td>\n",
              "      <td>-0.812596</td>\n",
              "      <td>-0.113876</td>\n",
              "      <td>0.661719</td>\n",
              "      <td>-0.586909</td>\n",
              "      <td>0.259515</td>\n",
              "      <td>-0.128141</td>\n",
              "      <td>0.663969</td>\n",
              "      <td>-0.515993</td>\n",
              "      <td>0.436046</td>\n",
              "      <td>-0.333097</td>\n",
              "      <td>0.102360</td>\n",
              "      <td>0.652380</td>\n",
              "      <td>-0.461109</td>\n",
              "      <td>0.027995</td>\n",
              "      <td>-0.125902</td>\n",
              "      <td>0.034412</td>\n",
              "      <td>1.048993</td>\n",
              "      <td>0.034781</td>\n",
              "      <td>0.435503</td>\n",
              "      <td>0.965708</td>\n",
              "      <td>0.199289</td>\n",
              "      <td>0.258940</td>\n",
              "      <td>-0.450650</td>\n",
              "      <td>0.066384</td>\n",
              "      <td>-0.065783</td>\n",
              "      <td>-0.649887</td>\n",
              "      <td>-0.114031</td>\n",
              "      <td>-0.131986</td>\n",
              "      <td>0.021596</td>\n",
              "      <td>0.520628</td>\n",
              "      <td>-0.056275</td>\n",
              "      <td>-0.183740</td>\n",
              "      <td>-1.301966</td>\n",
              "      <td>0.193906</td>\n",
              "      <td>0.106588</td>\n",
              "      <td>0.062884</td>\n",
              "      <td>0.063359</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.354675</td>\n",
              "      <td>-0.617412</td>\n",
              "      <td>0.919868</td>\n",
              "      <td>-0.882100</td>\n",
              "      <td>0.397901</td>\n",
              "      <td>0.397487</td>\n",
              "      <td>-0.379388</td>\n",
              "      <td>0.314216</td>\n",
              "      <td>0.100787</td>\n",
              "      <td>0.786078</td>\n",
              "      <td>-0.697638</td>\n",
              "      <td>0.121449</td>\n",
              "      <td>0.013268</td>\n",
              "      <td>0.239560</td>\n",
              "      <td>0.392050</td>\n",
              "      <td>-0.556672</td>\n",
              "      <td>0.719795</td>\n",
              "      <td>0.278738</td>\n",
              "      <td>-0.570492</td>\n",
              "      <td>0.837745</td>\n",
              "      <td>-0.194009</td>\n",
              "      <td>0.268559</td>\n",
              "      <td>0.380689</td>\n",
              "      <td>0.624278</td>\n",
              "      <td>-0.094663</td>\n",
              "      <td>-0.623941</td>\n",
              "      <td>0.133633</td>\n",
              "      <td>-0.217117</td>\n",
              "      <td>-0.672485</td>\n",
              "      <td>0.211056</td>\n",
              "      <td>1.322194</td>\n",
              "      <td>-0.119464</td>\n",
              "      <td>-0.388648</td>\n",
              "      <td>-0.748765</td>\n",
              "      <td>-0.199255</td>\n",
              "      <td>-0.382009</td>\n",
              "      <td>0.088932</td>\n",
              "      <td>-0.839256</td>\n",
              "      <td>0.784935</td>\n",
              "      <td>0.513740</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>-0.026921</td>\n",
              "      <td>-0.381943</td>\n",
              "      <td>1.107266</td>\n",
              "      <td>-0.429580</td>\n",
              "      <td>0.755200</td>\n",
              "      <td>-0.015292</td>\n",
              "      <td>-0.865784</td>\n",
              "      <td>0.681212</td>\n",
              "      <td>-0.146739</td>\n",
              "      <td>0.971243</td>\n",
              "      <td>-0.869253</td>\n",
              "      <td>0.158204</td>\n",
              "      <td>-1.006461</td>\n",
              "      <td>0.591428</td>\n",
              "      <td>0.781535</td>\n",
              "      <td>-0.288375</td>\n",
              "      <td>0.076625</td>\n",
              "      <td>0.338774</td>\n",
              "      <td>-0.468398</td>\n",
              "      <td>1.139519</td>\n",
              "      <td>0.426430</td>\n",
              "      <td>-0.685776</td>\n",
              "      <td>0.402645</td>\n",
              "      <td>-0.259806</td>\n",
              "      <td>-0.054182</td>\n",
              "      <td>-0.401925</td>\n",
              "      <td>-0.260160</td>\n",
              "      <td>-0.580926</td>\n",
              "      <td>-0.151273</td>\n",
              "      <td>-0.350646</td>\n",
              "      <td>1.168210</td>\n",
              "      <td>-0.291671</td>\n",
              "      <td>-0.126155</td>\n",
              "      <td>-0.602235</td>\n",
              "      <td>-0.296945</td>\n",
              "      <td>-0.798772</td>\n",
              "      <td>0.848600</td>\n",
              "      <td>-0.627088</td>\n",
              "      <td>0.538728</td>\n",
              "      <td>-0.287223</td>\n",
              "      <td>...</td>\n",
              "      <td>0.267439</td>\n",
              "      <td>0.249059</td>\n",
              "      <td>-0.030523</td>\n",
              "      <td>-0.085384</td>\n",
              "      <td>-0.040739</td>\n",
              "      <td>-0.081996</td>\n",
              "      <td>0.181996</td>\n",
              "      <td>-0.13307</td>\n",
              "      <td>0.006534</td>\n",
              "      <td>0.132713</td>\n",
              "      <td>-0.113397</td>\n",
              "      <td>-0.121518</td>\n",
              "      <td>0.201801</td>\n",
              "      <td>0.135771</td>\n",
              "      <td>0.258362</td>\n",
              "      <td>-0.250189</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>-0.245155</td>\n",
              "      <td>-0.197599</td>\n",
              "      <td>1.299455</td>\n",
              "      <td>-0.923318</td>\n",
              "      <td>-0.386898</td>\n",
              "      <td>0.808841</td>\n",
              "      <td>-0.498440</td>\n",
              "      <td>0.298925</td>\n",
              "      <td>0.311916</td>\n",
              "      <td>0.352545</td>\n",
              "      <td>-0.435687</td>\n",
              "      <td>0.287018</td>\n",
              "      <td>0.030146</td>\n",
              "      <td>0.344678</td>\n",
              "      <td>0.067195</td>\n",
              "      <td>-0.603182</td>\n",
              "      <td>0.035969</td>\n",
              "      <td>0.046298</td>\n",
              "      <td>-0.068767</td>\n",
              "      <td>0.504222</td>\n",
              "      <td>-0.619470</td>\n",
              "      <td>0.480948</td>\n",
              "      <td>0.983274</td>\n",
              "      <td>0.454100</td>\n",
              "      <td>0.032968</td>\n",
              "      <td>-0.265197</td>\n",
              "      <td>0.233669</td>\n",
              "      <td>-0.356425</td>\n",
              "      <td>-0.802311</td>\n",
              "      <td>0.291545</td>\n",
              "      <td>-0.199171</td>\n",
              "      <td>0.468158</td>\n",
              "      <td>0.094786</td>\n",
              "      <td>-0.587081</td>\n",
              "      <td>-0.471789</td>\n",
              "      <td>-1.244216</td>\n",
              "      <td>0.200717</td>\n",
              "      <td>0.218923</td>\n",
              "      <td>0.434786</td>\n",
              "      <td>0.467018</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>-0.245155</td>\n",
              "      <td>-0.197599</td>\n",
              "      <td>1.299455</td>\n",
              "      <td>-0.923318</td>\n",
              "      <td>-0.386898</td>\n",
              "      <td>0.808841</td>\n",
              "      <td>-0.498440</td>\n",
              "      <td>0.298925</td>\n",
              "      <td>0.311916</td>\n",
              "      <td>0.352545</td>\n",
              "      <td>-0.435687</td>\n",
              "      <td>0.287018</td>\n",
              "      <td>0.030146</td>\n",
              "      <td>0.344678</td>\n",
              "      <td>0.067195</td>\n",
              "      <td>-0.603182</td>\n",
              "      <td>0.035969</td>\n",
              "      <td>0.046298</td>\n",
              "      <td>-0.068767</td>\n",
              "      <td>0.504222</td>\n",
              "      <td>-0.619470</td>\n",
              "      <td>0.480948</td>\n",
              "      <td>0.983274</td>\n",
              "      <td>0.454100</td>\n",
              "      <td>0.032968</td>\n",
              "      <td>-0.265197</td>\n",
              "      <td>0.233669</td>\n",
              "      <td>-0.356425</td>\n",
              "      <td>-0.802311</td>\n",
              "      <td>0.291545</td>\n",
              "      <td>-0.199171</td>\n",
              "      <td>0.468158</td>\n",
              "      <td>0.094786</td>\n",
              "      <td>-0.587081</td>\n",
              "      <td>-0.471789</td>\n",
              "      <td>-1.244216</td>\n",
              "      <td>0.200717</td>\n",
              "      <td>0.218923</td>\n",
              "      <td>0.434786</td>\n",
              "      <td>0.467018</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40430</th>\n",
              "      <td>-0.245155</td>\n",
              "      <td>-0.197599</td>\n",
              "      <td>1.299455</td>\n",
              "      <td>-0.923318</td>\n",
              "      <td>-0.386898</td>\n",
              "      <td>0.808841</td>\n",
              "      <td>-0.498440</td>\n",
              "      <td>0.298925</td>\n",
              "      <td>0.311916</td>\n",
              "      <td>0.352545</td>\n",
              "      <td>-0.435687</td>\n",
              "      <td>0.287018</td>\n",
              "      <td>0.030146</td>\n",
              "      <td>0.344678</td>\n",
              "      <td>0.067195</td>\n",
              "      <td>-0.603182</td>\n",
              "      <td>0.035969</td>\n",
              "      <td>0.046298</td>\n",
              "      <td>-0.068767</td>\n",
              "      <td>0.504222</td>\n",
              "      <td>-0.619470</td>\n",
              "      <td>0.480948</td>\n",
              "      <td>0.983274</td>\n",
              "      <td>0.454100</td>\n",
              "      <td>0.032968</td>\n",
              "      <td>-0.265197</td>\n",
              "      <td>0.233669</td>\n",
              "      <td>-0.356425</td>\n",
              "      <td>-0.802311</td>\n",
              "      <td>0.291545</td>\n",
              "      <td>-0.199171</td>\n",
              "      <td>0.468158</td>\n",
              "      <td>0.094786</td>\n",
              "      <td>-0.587081</td>\n",
              "      <td>-0.471789</td>\n",
              "      <td>-1.244216</td>\n",
              "      <td>0.200717</td>\n",
              "      <td>0.218923</td>\n",
              "      <td>0.434786</td>\n",
              "      <td>0.467018</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40435</th>\n",
              "      <td>0.386772</td>\n",
              "      <td>-0.080047</td>\n",
              "      <td>0.335032</td>\n",
              "      <td>-0.191143</td>\n",
              "      <td>0.017317</td>\n",
              "      <td>0.800505</td>\n",
              "      <td>-0.569131</td>\n",
              "      <td>-0.707475</td>\n",
              "      <td>-0.183694</td>\n",
              "      <td>-0.111185</td>\n",
              "      <td>0.167191</td>\n",
              "      <td>-0.440074</td>\n",
              "      <td>-0.278192</td>\n",
              "      <td>0.888091</td>\n",
              "      <td>-0.259168</td>\n",
              "      <td>-1.043898</td>\n",
              "      <td>-0.357590</td>\n",
              "      <td>1.016953</td>\n",
              "      <td>-0.118375</td>\n",
              "      <td>-0.120694</td>\n",
              "      <td>-0.697720</td>\n",
              "      <td>-1.120563</td>\n",
              "      <td>0.629472</td>\n",
              "      <td>-0.381467</td>\n",
              "      <td>-0.338483</td>\n",
              "      <td>0.531732</td>\n",
              "      <td>0.705083</td>\n",
              "      <td>-0.366987</td>\n",
              "      <td>-0.982714</td>\n",
              "      <td>0.133510</td>\n",
              "      <td>0.587484</td>\n",
              "      <td>0.460253</td>\n",
              "      <td>-0.696820</td>\n",
              "      <td>0.354205</td>\n",
              "      <td>-0.453018</td>\n",
              "      <td>0.733362</td>\n",
              "      <td>0.235930</td>\n",
              "      <td>-0.338147</td>\n",
              "      <td>-0.161500</td>\n",
              "      <td>0.774563</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40440</th>\n",
              "      <td>0.231298</td>\n",
              "      <td>-0.295970</td>\n",
              "      <td>0.874799</td>\n",
              "      <td>-0.769696</td>\n",
              "      <td>0.396638</td>\n",
              "      <td>0.591152</td>\n",
              "      <td>-0.328180</td>\n",
              "      <td>0.332679</td>\n",
              "      <td>0.137527</td>\n",
              "      <td>0.613235</td>\n",
              "      <td>-0.727412</td>\n",
              "      <td>0.226585</td>\n",
              "      <td>-0.143604</td>\n",
              "      <td>0.197510</td>\n",
              "      <td>0.320635</td>\n",
              "      <td>-0.388608</td>\n",
              "      <td>0.681303</td>\n",
              "      <td>0.124423</td>\n",
              "      <td>-0.454745</td>\n",
              "      <td>0.549883</td>\n",
              "      <td>-0.152419</td>\n",
              "      <td>0.183227</td>\n",
              "      <td>0.388163</td>\n",
              "      <td>0.224696</td>\n",
              "      <td>-0.027964</td>\n",
              "      <td>-0.393427</td>\n",
              "      <td>-0.050577</td>\n",
              "      <td>-0.377115</td>\n",
              "      <td>-0.613030</td>\n",
              "      <td>0.069022</td>\n",
              "      <td>0.896258</td>\n",
              "      <td>0.094008</td>\n",
              "      <td>-0.145219</td>\n",
              "      <td>-0.525407</td>\n",
              "      <td>-0.167734</td>\n",
              "      <td>-0.372965</td>\n",
              "      <td>0.044405</td>\n",
              "      <td>-0.435962</td>\n",
              "      <td>0.528605</td>\n",
              "      <td>0.308131</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40445</th>\n",
              "      <td>0.078713</td>\n",
              "      <td>-0.014490</td>\n",
              "      <td>1.425323</td>\n",
              "      <td>-1.136940</td>\n",
              "      <td>-0.151528</td>\n",
              "      <td>1.020856</td>\n",
              "      <td>-0.178802</td>\n",
              "      <td>0.162451</td>\n",
              "      <td>0.555397</td>\n",
              "      <td>0.758140</td>\n",
              "      <td>-0.850079</td>\n",
              "      <td>0.571638</td>\n",
              "      <td>-0.275261</td>\n",
              "      <td>0.256533</td>\n",
              "      <td>0.482673</td>\n",
              "      <td>-0.277825</td>\n",
              "      <td>0.756893</td>\n",
              "      <td>-0.008256</td>\n",
              "      <td>-0.056403</td>\n",
              "      <td>0.252876</td>\n",
              "      <td>-0.612059</td>\n",
              "      <td>0.561979</td>\n",
              "      <td>1.129387</td>\n",
              "      <td>0.077244</td>\n",
              "      <td>0.005760</td>\n",
              "      <td>-0.485130</td>\n",
              "      <td>-0.155715</td>\n",
              "      <td>-0.321171</td>\n",
              "      <td>-0.860549</td>\n",
              "      <td>-0.050454</td>\n",
              "      <td>-0.170351</td>\n",
              "      <td>0.699564</td>\n",
              "      <td>0.119767</td>\n",
              "      <td>-0.680493</td>\n",
              "      <td>-0.236649</td>\n",
              "      <td>-0.821638</td>\n",
              "      <td>-0.296741</td>\n",
              "      <td>0.014397</td>\n",
              "      <td>0.485363</td>\n",
              "      <td>0.601624</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40450</th>\n",
              "      <td>-0.245155</td>\n",
              "      <td>-0.197599</td>\n",
              "      <td>1.299455</td>\n",
              "      <td>-0.923318</td>\n",
              "      <td>-0.386898</td>\n",
              "      <td>0.808841</td>\n",
              "      <td>-0.498440</td>\n",
              "      <td>0.298925</td>\n",
              "      <td>0.311916</td>\n",
              "      <td>0.352545</td>\n",
              "      <td>-0.435687</td>\n",
              "      <td>0.287018</td>\n",
              "      <td>0.030146</td>\n",
              "      <td>0.344678</td>\n",
              "      <td>0.067195</td>\n",
              "      <td>-0.603182</td>\n",
              "      <td>0.035969</td>\n",
              "      <td>0.046298</td>\n",
              "      <td>-0.068767</td>\n",
              "      <td>0.504222</td>\n",
              "      <td>-0.619470</td>\n",
              "      <td>0.480948</td>\n",
              "      <td>0.983274</td>\n",
              "      <td>0.454100</td>\n",
              "      <td>0.032968</td>\n",
              "      <td>-0.265197</td>\n",
              "      <td>0.233669</td>\n",
              "      <td>-0.356425</td>\n",
              "      <td>-0.802311</td>\n",
              "      <td>0.291545</td>\n",
              "      <td>-0.199171</td>\n",
              "      <td>0.468158</td>\n",
              "      <td>0.094786</td>\n",
              "      <td>-0.587081</td>\n",
              "      <td>-0.471789</td>\n",
              "      <td>-1.244216</td>\n",
              "      <td>0.200717</td>\n",
              "      <td>0.218923</td>\n",
              "      <td>0.434786</td>\n",
              "      <td>0.467018</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8091 rows × 1024 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           0         1         2         3     ...  1020  1021  1022  1023\n",
              "0     -0.021993 -0.333375  1.142767 -0.812596  ...   0.0   0.0   0.0   0.0\n",
              "5      0.354675 -0.617412  0.919868 -0.882100  ...   0.0   0.0   0.0   0.0\n",
              "10    -0.026921 -0.381943  1.107266 -0.429580  ...   0.0   0.0   0.0   0.0\n",
              "15    -0.245155 -0.197599  1.299455 -0.923318  ...   0.0   0.0   0.0   0.0\n",
              "20    -0.245155 -0.197599  1.299455 -0.923318  ...   0.0   0.0   0.0   0.0\n",
              "...         ...       ...       ...       ...  ...   ...   ...   ...   ...\n",
              "40430 -0.245155 -0.197599  1.299455 -0.923318  ...   0.0   0.0   0.0   0.0\n",
              "40435  0.386772 -0.080047  0.335032 -0.191143  ...   0.0   0.0   0.0   0.0\n",
              "40440  0.231298 -0.295970  0.874799 -0.769696  ...   0.0   0.0   0.0   0.0\n",
              "40445  0.078713 -0.014490  1.425323 -1.136940  ...   0.0   0.0   0.0   0.0\n",
              "40450 -0.245155 -0.197599  1.299455 -0.923318  ...   0.0   0.0   0.0   0.0\n",
              "\n",
              "[8091 rows x 1024 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QpYYekgcd10s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}