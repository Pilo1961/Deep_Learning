\documentclass[a4paper,11pt]{article}

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{xcolor}

\renewcommand\familydefault{\sfdefault}
\usepackage{tgheros}
\usepackage[defaultmono]{droidmono}

\usepackage{amsmath,amssymb,amsthm,textcomp}
\usepackage{enumerate}
\usepackage{multicol}
\usepackage{tikz}

\usepackage{geometry}
\geometry{left=25mm,right=25mm,%
bindingoffset=0mm, top=20mm,bottom=20mm}


\linespread{1.3}

\newcommand{\linia}{\rule{\linewidth}{0.5pt}}

% custom theorems if needed
\newtheoremstyle{mytheor}
    {1ex}{1ex}{\normalfont}{0pt}{\scshape}{.}{1ex}
    {{\thmname{#1 }}{\thmnumber{#2}}{\thmnote{ (#3)}}}

\theoremstyle{mytheor}
\newtheorem{defi}{Definition}

% my own titles
\makeatletter
\renewcommand{\maketitle}{
\begin{center}
\vspace{2ex}
{\huge \textsc{\@title}}
\vspace{1ex}
\\
\linia\\
\@author \hfill \@date
\vspace{4ex}
\end{center}
}
\makeatother
%%%

% custom footers and headers
\usepackage{fancyhdr}
\pagestyle{fancy}
\lhead{}
\chead{}
\rhead{}
\lfoot{Tarea \textnumero{} 9}
\cfoot{}
\rfoot{Page \thepage}
\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrulewidth}{0pt}
%

% code listing settings
\usepackage{listings}
\lstset{
    language=Python,
    basicstyle=\ttfamily\small,
    aboveskip={1.0\baselineskip},
    belowskip={1.0\baselineskip},
    columns=fixed,
    extendedchars=true,
    breaklines=true,
    tabsize=4,
    prebreak=\raisebox{0ex}[0ex][0ex]{\ensuremath{\hookleftarrow}},
    frame=lines,
    showtabs=false,
    showspaces=false,
    showstringspaces=false,
    keywordstyle=\color[rgb]{0.627,0.126,0.941},
    commentstyle=\color[rgb]{0.133,0.545,0.133},
    stringstyle=\color[rgb]{01,0,0},
    numbers=left,
    numberstyle=\small,
    stepnumber=1,
    numbersep=10pt,
    captionpos=t,
    escapeinside={\%*}{*)}
}

%%%----------%%%----------%%%----------%%%----------%%%

\begin{document}

\title{Deep Learning. Tarea 9. AutoEncoders}

\author{Sebastián Cadavid-Sánchez - Juan Pablo Herrera Musi}

\date{23 de Abril de 2020}

\maketitle

\section*{Pregunta 1}
Diseña un AE que obtenga pérdida menor o igual 0.01 tanto en entrenamiento como validación. Reporta tu arquitectura. 

\textbf{Solución:}

La mejor arquitectura AE entrenada obtuvo una pérdida de 0.0152 y 94.71\% de precisión para el conjunto de datos de entrenamiento, y  0.0101 y 96.16\%, respectivamente, para el conjunto de datos de test. En particular, la fase de entrenamiento asociada a dicho resultado incluyó 100 épocas, utilizando el optimizador "adam".  Esta arquitectura es la número 5 reportada en la parte final del notebook anexado.

En el \textit{listing} \ref{list:p1_arquitectura} se reporta el código utilizado para generar la mejor arquitectura y en la figura \ref{fig:p1_arquitectura} se presenta la arquitectura asociada.


    \begin{lstlisting}[label={list:p1_arquitectura},caption=Arquitectura con mejor desempeño]
    from tensorflow.keras import regularizers
    # Create model
    in_layer = Input(shape=(x_train.shape[1],))

    encoder = Dense(64, activation='relu')(in_layer)
    encoder = Dense(32,  activation='tanh',
    activity_regularizer=regularizers.l1(10e-5))(encoder)

    latent  = Dense(24,  activation='relu')(encoder)

    decoder = Dense(32, activation='relu',
    activity_regularizer=regularizers.l1(10e-5))(latent)
    decoder = Dense(64, activation='tanh')(decoder)

    out_layer = Dense(x_train.shape[1])(decoder)

    AE = Model(inputs=in_layer, outputs=out_layer)
    \end{lstlisting}

    \begin{center}
    \begin{figure}[h]
    \begin{centering}
    \includegraphics[scale=0.5]{img/p1_arquitectura.png}
    \par\end{centering}
    \caption{Arquitectura de mejor arquitectura con AE.  \label{fig:p1_arquitectura}}
    \end{figure}
    \par\end{center}


    \begin{center}
    \begin{figure}[h]
    \begin{centering}
    \includegraphics[scale=0.4]{img/p1_rendimiento.png}
    \par\end{centering}
    \caption{Pérdida y precisión de la Arquitectura de mejor arquitectura con AE para los datos de entrenamiento y prueba.  \label{fig:p1_arquitectura}}
    \end{figure}
    \par\end{center}
    
Como se puede observar en la arquitectura de mejor desempeño, los cambios más relevantes incluidos implicaron la utilización de regularización \textit{l1}, en el \textit{encoder} inicial que incluye una función de activación de tangente hiperbólica, y la que precede al último \textit{decoder}, que posee una función de activación \textit{relu}. De igual forma, la última capa \textit{decoder} incorpora una función de activación de tangente hiperbólica.

Cabe mencionar que, si se observa la figura \ref{fig:p1_arquitectura} el proceso de aprendizaje para los datos de prueba es mucho menos suavizado que para el conjunto de datos de entrenamiento. Sin embargo, no hay señales de sobre-entrenamiento, en la medida que las curvas tanto de pérdida como de precisión se comportan de manera similar. 

En los incisos posteriores se discutirá sobre algunos de los detalles sobre las arquitecturas de modelos de \textit{autoencoding}.
    
\section*{Pregunta 2}

¿Existe alguna relación entre la profundidad del AE y la pérdida final?


\textbf{Solución:}  

No existe ninguna relación directa entre la profundidad de autoencoder y la pérdida final. La pérdida final solamente toma en cuenta las diferencias entre el \textit{input} y el \textit{output} y no tiene dependencia con la profundidad del autoencoder. Por ejemplo, si la función de pérdida es la de error cuadrado medio, tenemos que: 
$$loss=||x-\hat x||^2 =||x-d(z)||^2= ||x-d(e(x))||^2$$  
con:
\begin{itemize}
    \item $x$ el \textit{input} 
    \item $\hat x$ el \textit{output}, que corresponde a la reconstrucción de \textit{input}
    \item $z$ es la representación de $x$ en el espacio latente
    \item $d(\cdot)$ es el \textit{decoder}, una función que transforma del espacio latente al espacio original
    \item $e(\cdot)$ es el \textit{encoder}, una función que transforma del espacio orogonal al espacio latente
\end{itemize}

Entendemos la profundidad como el número de capas que tiene el encoder antes de alcanzar la capa latente. La relevancia de la profundidad se verá reflejada en las diferentes características del \textit{input} que el encoder es capaz transformar sin perder información. La profundidad nos indicará el número de grados de libertad que se tienen para lograr reducciones de dimensionalidad considerables. Sin embargo no tiene relación directa con la función de pérdida.


\section*{Pregunta 3}

¿Existe alguna relación entre la profundidad del AE y la separación resultante entre los espacios latentes de los datos normales y anormales?

\textbf{Solución:}  

Si existe una relación entre la profundidad del AE y la separación resultante entre los espacios latentes de los dos tipos de datos. El modelo planteado por un autoencoder lleva los datos representados un la dimensión original a una representación en el espacio latente con diferentes dimensiones. Intuitivamente la naturaleza de la arquitectura de red que está entre la capa de entrada y la capa latente determina las transformaciones que se le hacen a los datos de entrada y la representación final que tiene los datos de salida. Dependiendo de los parámetros y número de capas, la red aprenderá características diferentes de los datos y determinará la representación que se le da a los mismos en el espacio latente.

Otra forma de pensar el problema es con el planteamiento matemático del mismo. Podemos simplificar el problema y pensar todo el proceso que realiza el encoder para trasformar el \textit{input} lo podemos modelar como una función $\phi:\chi \rightarrow{} F$, con $\chi$ el espacio original y $F$ el espacio latente. Si cambiamos la regla de correspondencia $\phi$ entonces se generará un \textit{output} diferente aún cuando vive en el mismo espacio. Lo que hacemos al modificar la profundidad es cambiar la función $\phi$.  

En términos de la red neuronal la función $\phi$ tendrá la forma de $\phi(\textbf{x})=\sigma(\textbf{W}x+\textbf{b})$ con $\textbf{x} \in \chi$ y $\phi(\textbf{x}) \in F$ para encoders con 1 capa de profundidad. Si aumentamos la profundidad entonces tendremos composiciones de funciones $\phi(\phi_1(\textbf{x}))$ y aunque el vector final viva en el mismo espacio tendrá una representación diferente.

En la figura \ref{fig:p3-mas-capas} observamos el espacio latente de una red con 5 capas. En la figura \ref{fig:p3-menos-capas} observamos el espacio latente de un red con 3 capas. Es posible observar que ambos espacios son muy similares, sin embargo podemos apreciar diferencias sobre todo los puntos que están por las orillas. El motivos de que son diferente se explica en párrafos anteriores, el gran parecido se debe a que están encontrando las mismas características y por tanto representaciones similares.

    \begin{center}
    \begin{figure}[h]
    \begin{centering}
    \includegraphics[scale=0.5]{img/p3-mas_capas.PNG}
    \par\end{centering}
    \caption{Espacio latente de una red con 5 capas. \label{fig:p3-mas-capas}}
    \end{figure}
    \par\end{center}
    
    
    \begin{center}
    \begin{figure}[h]
    \begin{centering}
    \includegraphics[scale=0.5]{img/p3-menoscapas.PNG}
    \par\end{centering}
    \caption{Espacio latente de una red con 3 capas. \label{fig:p3-menos-capas}}
    \end{figure}
    \par\end{center}
    
\section*{Pregunta 4}

¿Existe alguna relación entre el número elementos en el espacio latente y la pérdida final?

\textbf{Solución:}

Si, existe una relación indirecta entre el número de elementos en el espacio latente y la pérdida final. Como mencionamos en la pregunta dos, la pérdida final es una función que solamente depende del \textit{input} y el \textit{output}, pero la pérdida contabiliza diferencias entre el input y el output. Por otro lado, el número de elementos en el espacio latente indica la cantidad de información que podremos almacenar de un \textit{input} codificado. Si tenemos muy pocas dimensiones en ese espacio es muy posible que tengamos mucha pérdida de información y al reconstruir la salida tendrá diferencias considerables con el \textit{input}, esto se verá reflejado al evaluar la pérdida. 

Una dimensión del espacio latente mayor o igual a la dimensión de entrada permitirá almacenar la información completa del \textit{input}, por lo que esperariamos que se tenga menos pérdida de información y menos pérdida final. 

Para verificar la información se hicieron pruebas con cuatro redes en las que la única variación era el tamaño de la capa latente. La figura \ref{fig:arq-p2} muestra una de estas representaciones. 

    \begin{center}
    \begin{figure}[h]
    \begin{centering}
    \includegraphics[scale=0.5]{img/preg2-arq-red.png}
    \par\end{centering}
    \caption{Arquitectura de red de prueba. \label{fig:arq-p2}}
    \end{figure}
    \par\end{center}

En las otras redes el único hiperparámetro que se modificó fue la dimensión de la capa latente. Se hicieron pruebas con dimensiones 4, 8, 16 y 32. Los resultados se resumen en la tabla \ref{table:tabla-preg-4}. 

\begin{table}[]
\centering
 \begin{tabular}{||c c c ||} 
 \hline
 Dimensión Espacio Latente & Pérdida Test & Pérdida Val  \\ [0.5ex] 
 \hline\hline
 4 & 0.2252 & 0.2373  \\ 
 \hline
 8 & 0.0909 & 0.0981  \\
 \hline
 16 & 0.0136 & 0.0205  \\
 \hline
 32 & 0.0050 & 0.0088  \\
 \hline
\end{tabular}
\caption{Pruebas de variación de profundidad. \label{table:tabla-preg-4}}
\end{table}

\section*{Pregunta 5}

¿Existe alguna relación entre el número elementos en el espacio latente y la separación resultante entre los espacios latentes de los datos normales y anormales?

\textbf{Solución:}

Si, existe una relación directa entre el número de elementos en el espacio latente y la separación resultante entre los datos normales y anormales. El número de elementos en el espacio latente determina cuantas separaciones se harán dentro de nuestros datos, por tanto si cambiamos el número de elementos en el espacio latente la separación de los datos cambiará.  

Por ejemplo, si utilizamos la misma arquitectura que de 3 capas que se utilizó en la pregunta 3, cuyo espacio latente observamos en la figura \ref{fig:p3-menos-capas} pero cambiamos la dimensión de la capa latente de 16 a 8, entonces observamos el espacio latente que se muestra en la figura \ref{fig:p5-espacio-latente}. El número de divisiones que tiene el espacio latente en este caso es de 8, en el caso anterior era de 16. De modo que la separación de los datos en el espacio latente sí depende de el número de elementos en el mismo.

    \begin{center}
    \begin{figure}[h]
    \begin{centering}
    \includegraphics[scale=0.5]{img/p5-espacio-latente.PNG}
    \par\end{centering}
    \caption{Espacio latente de 8 elementos. \label{fig:p5-espacio-latente}}
    \end{figure}
    \par\end{center}


\textbf{Nota explicativa: Suponemos que la separación de elementos se refiere a su representación en el espacio latente. Si se refiere a transacciones fraudulentas o normales toda la división proviene de que nunca se juntan los datos, por un lado se introducen los fraudulentos y por otro los normales. De forma que nunca etiqueta si es fraudulento o normal.}


\section*{Pregunta 6}

Intenta forzar el espacio latente para que sea ralo. Reporta tu mejor modelo y el desempeño que obtenga, tanto en pérdida como en capacidad de diferenciar datos anormales.

\textbf{Solución:}


En el \textit{listing} \ref{list:p6_arquitectura} se reporta el código utilizado para generar la mejor arquitectura y en la figura \ref{fig:p6_arquitectura} se presenta la arquitectura asociada.

    \begin{lstlisting}[label={list:p6_arquitectura},caption=Arquitectura con mejor desempeño]
# Create model
    in_layer = Input(shape=(x_train.shape[1],))

    encoder = Dense(32, activation='relu')(in_layer)
    encoder = Dense(24,  activation='relu')(encoder)
    latent  = Dense(32,activation='relu',activity_regularizer=l1(0.1))(encoder)
    decoder = Dense(24, activation='relu')(latent)
    decoder = Dense(32, activation='relu')(decoder)
    out_layer = Dense(x_train.shape[1])(decoder)

    AE = Model(inputs=in_layer, outputs=out_layer)
    AE.summary()

# Compile
    AE.compile(optimizer='adam', loss='mse', metrics=['accuracy'])

# Train it
    history = AE.fit(x_train, x_train, epochs=30, batch_size=64, validation_split=0.2)
    \end{lstlisting}

    \begin{center}
    \begin{figure}[h]
    \begin{centering}
    \includegraphics[scale=0.5]{img/preg2-arq-red.png}
    \par\end{centering}
    \caption{Arquitectura de mejor arquitectura con AE.  \label{fig:p6_arquitectura}}
    \end{figure}
    \par\end{center}
En esta arquitectura se obligó a tener un espacio latente ralo. Para lograrlo se utilizó un regularizador LASSO ($l1$) en la capa latente. Se utilizó un factor de regularización de $0.1$, mayor al recomendado de $0.01$ con el fin de lograr el efecto esperado en el espacio latente. Forzar ese efecto también afectó la pérdida considerablemente, la pérdida final fue de $0.2214$ para entrenamiento y de $.4651$ para validación. 

En la figura \ref{fig:p6-espacio-latente} podemos observar que el espacio latente tiene varias posiciones en las que no se representa ningún valor. Sin embargo, la separación entre valores anormales es distinguible. Esto lo podemos observar en la gráfica del error cuadrático medio que se presenta en la figura \ref{fig:p6-separacion}.

\begin{center}
    \begin{figure}[h]
    \begin{centering}
    \includegraphics[scale=0.5]{img/p6-latent.PNG}
    \par\end{centering}
    \caption{Representación de los datos en el espacio latente para la arquitectura de la figura \ref{fig:p6_arquitectura}.  \label{fig:p6-espacio-latente}}
    \end{figure}
    \par\end{center}

\begin{center}
    \begin{figure}[h]
    \begin{centering}
    \includegraphics[scale=0.5]{img/p6-separacion.PNG}
    \par\end{centering}
    \caption{Separación de los datos utilizando la arquitectura de la figura \ref{fig:p6_arquitectura}.  \label{fig:p6-separacion}}
    \end{figure}
    \par\end{center}

\end{document}
